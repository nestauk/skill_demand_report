{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the stock of vacancies\n",
    "This script analyses the re-weighted stock of vacancies broken down by occupation, industry, location and skill category. It also does an analysis of location quotients by Travel To Work Areas (with associated Gini Indices) when analysing the breakdown by 'industry and location' and 'skill category and location'. \n",
    "\n",
    "\n",
    "A copy of the report can be found in this github repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Please forgive the less than ideal coding practices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ DEPENDENCIES AND FUNCTIONS ------------------------\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import folium\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from time import time as tt\n",
    "from shapely import geometry\n",
    "from scipy.special import gammaln\n",
    "import statsmodels.api as sm\n",
    "import json\n",
    "\n",
    "from utils_general import TaskTimer, print_elapsed, nesta_colours, sic_letter_to_text, flatten_lol, printdf, socnames_dict\n",
    "from textkernel_load_utils import tk_params, DATA_PATH, create_tk_import_dict, read_and_append_chunks\n",
    "from flow_to_stock_funcs import get_stock_breakdown, load_ons_vacancies\n",
    "from maputils_pin import draw_map\n",
    "\n",
    "timer = TaskTimer()\n",
    "print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with all the figures for the report\n",
    "output_folder = '/path/to/outputs'\n",
    "local_data_path = '/path/to/local/data'\n",
    "\n",
    "DATE_ID = 'July2020'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEFIG = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use more colours\n",
    "aug_nesta_colours = nesta_colours + [[1., 0., 0.], [0., 0., 1.], [0.,1.,0.], [.7, .7, .7]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIC mapping between text and letters\n",
    "\n",
    "sic_letter_to_text['Z'] = 'Others'\n",
    "sic_letter_to_text['L_O_S'] = 'Personal and public services'#including non-profit and estate agents\n",
    "sic_letter_to_text['D_E'] = 'Utilities (energy, water and waste)'\n",
    "sic_letter_to_text['M_P'] = 'Educational and professional activities'\n",
    "sic_letter_to_text['uncertain'] = 'Uncertain'\n",
    "\n",
    "sic_text_to_letter= {}\n",
    "for letter in sic_letter_to_text.keys():\n",
    "    sic_text= sic_letter_to_text[letter]\n",
    "    sic_text_to_letter[sic_text] = letter\n",
    "\n",
    "sic_letter_to_text_abbr = deepcopy(sic_letter_to_text)\n",
    "sic_letter_to_text_abbr['T'] = 'Activities of households as employers'\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_soc_to_n_digits(soc_code,n=3):\n",
    "    \"\"\" Resample SOC codes from 4 to n-digits\"\"\"\n",
    "    if np.isnan(soc_code):\n",
    "        return np.nan\n",
    "    else:\n",
    "        m = {1: 1000, 2: 100, 3: 10}[n]\n",
    "        return (soc_code - soc_code%m)/m\n",
    "\n",
    "def resample_soc_to_n_digits_df(soc_code_df,n=3):\n",
    "    \"\"\" Resample SOC codes from 4 to n-digits for a dataframe\"\"\"\n",
    "    m = {1: 1000, 2: 100, 3: 10}[n]\n",
    "    return (soc_code_df - soc_code_df%m)/m\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------------------------------------------------------------------------\n",
    "#              Main functions to convert from flow to stock\n",
    "#% --------------------------------------------------------------------------\n",
    "\n",
    "''' \n",
    "def set_month_at_beginning(x):\n",
    "    \"\"\"Set a datetime to the beginning of the month\"\"\"\n",
    "    return pd.offsets.MonthBegin(0).rollback(x)\n",
    "\n",
    "def set_month_at_end(x):\n",
    "    \"\"\" Set a datetime to the end of the month\"\"\"\n",
    "    return pd.offsets.MonthEnd(0).rollforward(x)\n",
    "    \n",
    "def get_stock_breakdown(data, agg_func = 'sum', agg_col = 'vacancy_weight', \n",
    "                              breakdown_col = 'organization_industry_value', BOUNDARY = None):\n",
    "    \"\"\"Compute the daily stock of vacancies via cumulative sum of net Flow.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- dataframe with online job vacancies. Need to have \"date\", \n",
    "            \"end_date\" and agg_col columns\n",
    "    agg_func: whether to count the vacancies or to sum the weights\n",
    "    agg_col -- reference column to aggregate (usually column with per-vacancy weights)\n",
    "    BOUNDARY -- what to do wrt boundary conditions (start and end month)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(breakdown_col,list):\n",
    "        breakdown_col = [breakdown_col]\n",
    "        \n",
    "    start_day = data.date.min()\n",
    "    end_day = data.date.max()\n",
    "    \n",
    "    print(agg_func)\n",
    "    if agg_func == 'sum':\n",
    "        vacancy_flow_per_day = data.groupby(['date'] + breakdown_col)[agg_col].sum()\n",
    "        vacancy_remove_per_day = data.groupby(['end_date']+ breakdown_col)[agg_col].sum()\n",
    "    else:\n",
    "        vacancy_flow_per_day = data.groupby(['date'] + breakdown_col)[agg_col].count()\n",
    "        vacancy_remove_per_day = data.groupby(['end_date'] + breakdown_col)[agg_col].count()\n",
    "    \n",
    "    for _ in range(len(breakdown_col)):\n",
    "        vacancy_flow_per_day = vacancy_flow_per_day.unstack()\n",
    "        vacancy_remove_per_day = vacancy_remove_per_day.unstack()\n",
    "    \n",
    "    # shift vacancy_remove_per_day by one day since vacancies disappear the day\n",
    "    # after their expiration date\n",
    "    vacancy_remove_per_day = vacancy_remove_per_day.shift(1)\n",
    "    \n",
    "    # adjust so that they start and end on the same dates\n",
    "    vacancy_flow_per_day = vacancy_flow_per_day.reindex(pd.date_range(start=start_day,\n",
    "                                        end=end_day,freq='D'), fill_value=0)#, level=0)\n",
    "    vacancy_remove_per_day = vacancy_remove_per_day.reindex(pd.date_range(start=start_day,\n",
    "                                        end=end_day,freq='D'), fill_value=0)#, level=0)\n",
    "    \n",
    "    # compute the net Flow\n",
    "    net_flow = vacancy_flow_per_day.fillna(0) - vacancy_remove_per_day.fillna(0)\n",
    "    \n",
    "    # Get the daily stock\n",
    "    daily_stock = net_flow.cumsum()\n",
    "    \n",
    "    # Resample to monthly stock\n",
    "    monthly_stock = net_flow.resample('M').sum().cumsum()/2\n",
    "    monthly_stock.index = monthly_stock.index.map(set_month_at_beginning)\n",
    "    \n",
    "    # enforce boundary conditions\n",
    "    if BOUNDARY == 'valid':\n",
    "        monthly_stock = monthly_stock[monthly_stock.index>=set_month_at_beginning(\n",
    "            pd.to_datetime(FIRST_VALID_MONTH))]\n",
    "    return monthly_stock, daily_stock, vacancy_flow_per_day, vacancy_remove_per_day\n",
    "\n",
    "def load_ons_vacancies(start_date = '2015-03-01', end_date = '2019-10-31'):\n",
    "    ons_df = pd.read_excel(f\"{DATA_PATH}/data/aux/x06apr20.xls\", sheet_name='Vacancies by industry',\n",
    "                          skiprows = 3)\n",
    "    # keep columns with data and clean the column names\n",
    "    ons_df = ons_df[[col for col in ons_df.columns if not 'Unnamed:' in col]]\n",
    "    cleaned_col_names = {}\n",
    "    for col in ons_df.columns[2:]:\n",
    "        cleaned_col = col.replace('&', 'and').replace('-','').replace(',','').lower()\n",
    "        cleaned_col = ''.join([t for t in cleaned_col if not t.isdigit()])\n",
    "        cleaned_col_names[col] = '_'.join(cleaned_col.split())\n",
    "    # manual adjustment for one column\n",
    "    cleaned_col_names['Manu-    facturing'] = 'manufacturing'\n",
    "    cleaned_col_names['SIC 2007 sections'] = 'month'\n",
    "    cleaned_col_names['All vacancies1 '] = 'vacancies'\n",
    "    ons_df = ons_df.rename(columns = cleaned_col_names)\n",
    "    # extract the row with the letters\n",
    "    sic_letters = ons_df.iloc[0]\n",
    "    # remove empty rows\n",
    "    ons_df = ons_df.loc[(ons_df.month.notna()) & (ons_df.vacancies.notna())]\n",
    "    # join up some industries\n",
    "    ons_df = ons_df.assign(wholesale_retail_motor_trade_and_repair = \n",
    "                           ons_df.motor_trades + ons_df.wholesale + ons_df.retail)\n",
    "    ons_df = ons_df.assign(wholesale_and_retail = ons_df.wholesale + ons_df.retail)\n",
    "    \n",
    "    ons_df = ons_df.assign(education_and_professional_activities = ons_df.education + \n",
    "                                                        ons_df.professional_scientific_and_technical_activities)\n",
    "    ons_df = ons_df.assign(utilities = ons_df.electricity_gas_steam_and_air_conditioning_supply + \n",
    "                           ons_df.water_supply_sewerage_waste_and_remediation_activities)\n",
    "    ons_df = ons_df.assign(personal_and_public_services = ons_df.real_estate_activities + \n",
    "                                                    ons_df['public_admin_and_defence;_compulsory_social_security'] +\n",
    "                                                    ons_df.other_service_activities)\n",
    "    sic_letters.loc['wholesale_retail_motor_trade_and_repair'] = 'G'\n",
    "    sic_letters.loc['wholesale_and_retail'] = 'G46_47'\n",
    "    sic_letters.loc['education_and_professional_activities'] = 'M_P'\n",
    "    sic_letters.loc['utilities'] = 'D_E'\n",
    "    sic_letters.loc['personal_and_public_services'] = 'L_O_S'\n",
    "    sic_letters.loc['others'] = 'Z'\n",
    "    sic_letters.loc['vacancies'] = 'vacancies'\n",
    "    #\n",
    "    ons_df.month = pd.to_datetime(ons_df.month)\n",
    "    # only need vacancies within a certain period\n",
    "    ons_df = ons_df[(ons_df.month>=pd.to_datetime(start_date)) & \n",
    "                    (ons_df.month<=pd.to_datetime(end_date))]\n",
    "    ons_df = ons_df.set_index('month')\n",
    "    return ons_df, sic_letters\n",
    "'''\n",
    "print('Uncomment if from flow_to_stock_funcs import get_stock_breakdown does not work')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All credit for this function goes to https://github.com/oliviaguest/gini/blob/master/gini.py\n",
    "def gini(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # based on bottom eq:\n",
    "    # http://www.statsdirect.com/help/generatedimages/equations/equation154.svg\n",
    "    # from:\n",
    "    # http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    # All values are treated equally, arrays must be 1d:\n",
    "    array = array.flatten()\n",
    "    if np.amin(array) < 0:\n",
    "        # Values cannot be negative:\n",
    "        array -= np.amin(array)\n",
    "    # Values cannot be 0:\n",
    "    array += 0.0000001\n",
    "    # Values must be sorted:\n",
    "    array = np.sort(array)\n",
    "    # Index per array element:\n",
    "    index = np.arange(1,array.shape[0]+1)\n",
    "    # Number of array elements:\n",
    "    n = array.shape[0]\n",
    "    # Gini coefficient:\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job adverts dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Usually would load Textkernel dataset')\n",
    "data_df = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data on TTWAs and other geographies\n",
    "\n",
    "1. TTWA dictionary to go from code to name\n",
    "2. TTWA statistics\n",
    "3. TTWA shapefiles\n",
    "4. Regions shapefiles\n",
    "5. Country shapefiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttwa_folder = '/path/to/ttwa/data/ONS/Travel_to_Work_Areas_2011_guidance_and_information_V4'\n",
    "ttwa_file_1 = 'TTWA-info-2016.xls'\n",
    "ttwa_file_2 = 'TTWA-summary-statistics-2011.xls'\n",
    "ttwa_stats = pd.read_excel(f\"{ttwa_folder}/{ttwa_file_1}\")\n",
    "print('TTWA stats from 2016')\n",
    "print(ttwa_stats.columns)\n",
    "tmp = pd.read_excel(f\"{ttwa_folder}/{ttwa_file_2}\")\n",
    "print('--------')\n",
    "print('TTWA stats from 2011')\n",
    "print(tmp.columns)\n",
    "ttwa_stats = ttwa_stats.merge(tmp, on =['ttwa11cd','ttwa11nm'], how= 'left')\n",
    "ttwa_code2name = dict(zip(ttwa_stats.ttwa11cd,ttwa_stats.ttwa11nm))\n",
    "ttwa_name2code = dict(zip(ttwa_stats.ttwa11nm,ttwa_stats.ttwa11cd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttwa_name2code['London']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ttwas_codes = data_df.ttwa11cd.value_counts().index.to_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_code = 'E30000234'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_ttwas = ttwa_stats.ttwa11cd[ttwa_stats[' Population']>60000].to_list() + [ttwa_name2code['Belfast']]\n",
    "london_code in biggest_ttwas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load official stastistics of employment levels (economically active residents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "employment_levels = pd.read_csv(f\"{local_data_path}/aux/economically_active_15_19_16plus_by_ttwa.csv\", skiprows=6)\n",
    "employment_levels = employment_levels.rename(columns={'mnemonic': 'ttwa11cd'}).set_index('ttwa11cd').iloc[:-3]\n",
    "employment_levels = employment_levels[[t for t in employment_levels.columns if 'Jan' in t]]\n",
    "employment_levels = employment_levels.rename(columns=\n",
    "                                {t: f\"employment_{t[-4:]}\" for t in employment_levels.columns if 'Jan' in t})\n",
    "employment_levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add statistics for Northern Ireland\n",
    "# No data is available for TTWAs - so, get statistics ...\n",
    "# ...at the country level and project on TTWAs based on the distribution by TTWA as obtained from the census\n",
    "NI_col = 'Number of economically active residents (aged 16+)'\n",
    "NI_ttwa = ttwa_stats[ttwa_stats['Region/Country']=='Northern Ireland'][[NI_col,'ttwa11cd']]\n",
    "NI_active = pd.read_csv(f\"{local_data_path}/aux/economically_active_16plus_11_15_19_NI.csv\", skiprows=7)\n",
    "NI_active = NI_active.iloc[1:]\n",
    "NI_active['Date'] = NI_active['Date'].map(lambda x: x[-4:])\n",
    "NI_active = NI_active.set_index('Date')\n",
    "NI_ratio_2011 = {}\n",
    "for year in ['2015','2016','2017','2018','2019']:\n",
    "    NI_ratio_2011[year] = float(NI_active.loc[year,'Northern Ireland'])/float(\n",
    "        NI_active.loc['2011','Northern Ireland'])\n",
    "    NI_ttwa[f\"employment_{year}\"] = NI_ttwa[NI_col]*NI_ratio_2011[year]\n",
    "NI_ttwa = NI_ttwa.set_index('ttwa11cd')\n",
    "NI_ttwa = NI_ttwa[[t for t in NI_ttwa.columns if 'employment' in t]]\n",
    "printdf(NI_ttwa.head())\n",
    "employment_levels = pd.concat((employment_levels,NI_ttwa))\n",
    "employment_levels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select TTWAs with population size larger than specific thresholds. How big are these subsets of TTWAs?\n",
    "biggest_employment_ttwas40 = employment_levels[employment_levels.mean(axis=1)>40000].index.to_list()\n",
    "print(len(biggest_employment_ttwas40))\n",
    "print(len(biggest_ttwas))\n",
    "biggest_employment_ttwas50 = employment_levels[employment_levels.mean(axis=1)>50000].index.to_list()\n",
    "print(len(biggest_employment_ttwas50))\n",
    "biggest_employment_ttwas250 = employment_levels[employment_levels.mean(axis=1)>250000].index.to_list()\n",
    "print(london_code in biggest_employment_ttwas250, london_code in biggest_employment_ttwas40)\n",
    "print(ttwa_name2code['Belfast'] in biggest_employment_ttwas250)\n",
    "[ttwa_code2name[t] for t in biggest_employment_ttwas250]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load information on most representative cluster for each job advert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release memory\n",
    "data_df += ['most_representative_cluster']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SOC to skill cluster crosswalks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gzip.GzipFile(f\"{DATA_PATH}/data/aux/final_all_crosswalks_soc_to_clusters_top_avg_{DATE_ID}.gz\",'rb') as f:\n",
    "    soc_by_clusters = pickle.load(f)\n",
    "\n",
    "# make sure everything is a float and not an int\n",
    "for k1 in soc_by_clusters.keys():\n",
    "    for k2 in soc_by_clusters[k1].keys():\n",
    "        soc_by_clusters[k1][k2] = soc_by_clusters[k1][k2]+0.0\n",
    "\n",
    "# normalise all crosswalk so that it sums up to 1 for each occupation\n",
    "timer.start_task('normalising crosswalks')\n",
    "soc_by_clusters_norm = {}\n",
    "for year in soc_by_clusters.keys():\n",
    "    soc_by_clusters_norm[year] = {}\n",
    "    for k in soc_by_clusters[year].keys():\n",
    "        tmp = deepcopy(soc_by_clusters[year][k])\n",
    "        tmp = tmp/tmp.sum()\n",
    "        soc_by_clusters_norm[year][k] = tmp\n",
    "        \n",
    "timer.end_task()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load information on the skills taxonomy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mapping from job adverts skills to ESCO-based skill clusters\n",
    "RESULTS_PATH = '/path/to/skills/taxonomy'\n",
    "validated_matches_file = f'tk_skills_to_skills_and_clusters_validated_final_{DATE_ID}.csv'\n",
    "final_matches_file = f'tk_skills_to_clusters_1to1_final_{DATE_ID}.csv'\n",
    "\n",
    "tk_esco_121 = pd.read_csv(f\"{RESULTS_PATH}/{final_matches_file}\", encoding = 'utf-8')\n",
    "tk_esco_121 = tk_esco_121.set_index('skill_label')\n",
    "\n",
    "# turn dataframe into dictionary\n",
    "tk_esco_121_dict = dict(zip(tk_esco_121.index,tk_esco_121.cluster_level_3))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the ESCO-based skill clusters\n",
    "# Load full clusters\n",
    "esco_clusters = pd.read_csv(esco_clusters_file)\n",
    "# make alt labels list\n",
    "esco_clusters['alt_labels'] = esco_clusters.alt_labels.map(\n",
    "        lambda x: x.split('\\n') if isinstance(x,str) else [])\n",
    "\n",
    "# adjustments to the labels\n",
    "esco_clusters.loc[esco_clusters.level_2==20.0,'label_level_2'] = 'land transport (rail)'\n",
    "esco_clusters.loc[esco_clusters.level_3==191.0,'label_level_3'\n",
    "                 ] = 'leather production (manufacturing)'\n",
    "esco_clusters.loc[esco_clusters.level_3==190.0,'label_level_3'] = 'footwear design'\n",
    "esco_clusters.loc[esco_clusters.level_3==57.0,'label_level_3'] = 'marketing (branding)'\n",
    "\n",
    "print('All ESCO skills: ', len(esco_clusters))\n",
    "print('Valid ESCO skills: ',len(esco_clusters[esco_clusters.level_1<15]))\n",
    "\n",
    "print(esco_clusters.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cluster labels\n",
    "cluster_labels_1 = pd.read_csv(f\"{esco_clusters_dir}/ESCO_Essential_clusters_Level_1.csv\")\n",
    "cluster_labels_2 = pd.read_csv(f\"{esco_clusters_dir}/ESCO_Essential_clusters_Level_2.csv\")\n",
    "cluster_labels_3 = pd.read_csv(f\"{esco_clusters_dir}/ESCO_Essential_clusters_Level_3.csv\")\n",
    "\n",
    "cluster_labels_3 = cluster_labels_3.set_index('level_3')\n",
    "cluster_labels_2 = cluster_labels_2.set_index('level_2')\n",
    "\n",
    "# correct for same labels!!\n",
    "cluster_labels_3.loc[191,'label'] = 'leather production (manufacturing)'\n",
    "cluster_labels_3.loc[190,'label'] = 'footwear design'\n",
    "cluster_labels_3.loc[57,'label'] = 'marketing (branding)'\n",
    "cluster_labels_2.loc[20,'label'] = 'land transport (rail)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the crosswalks between levels in the skills taxonomy\n",
    "esco_first_to_second={}\n",
    "esco_second_to_first={}\n",
    "for name,g in esco_clusters.groupby('level_1').level_2:#.value_counts():\n",
    "    level_2_all = sorted(g.value_counts().index.to_list())\n",
    "    esco_first_to_second[name] = level_2_all\n",
    "    for level_id in level_2_all:\n",
    "        esco_second_to_first[level_id] = name\n",
    "    \n",
    "esco_second_to_third={}\n",
    "esco_third_to_second = {}\n",
    "esco_third_to_first = {}\n",
    "for name,g in esco_clusters.groupby('level_2').level_3:#.value_counts():\n",
    "    level_3_all = sorted(g.value_counts().index.to_list())\n",
    "    esco_second_to_third[name] = level_3_all\n",
    "    for level_id in level_3_all:\n",
    "        esco_third_to_second[level_id] = name\n",
    "        esco_third_to_first[level_id] = esco_second_to_first[name]\n",
    "\n",
    "esco_first_to_third = {}\n",
    "for name in esco_first_to_second.keys():\n",
    "    level_2_all = esco_first_to_second[name]\n",
    "    level_3_all = []\n",
    "    for level_id in level_2_all:\n",
    "        level_3_all.append(esco_second_to_third[level_id])\n",
    "    esco_first_to_third[name] = sorted(flatten_lol(level_3_all))\n",
    "\n",
    "print('done')\n",
    "\n",
    "# same but with the levels\n",
    "esco_first_to_second_label={}\n",
    "esco_second_to_first_label={}\n",
    "for name,g in esco_clusters.groupby('label_level_1').label_level_2:#.value_counts():\n",
    "    level_2_all = sorted(g.value_counts().index.to_list())\n",
    "    esco_first_to_second_label[name] = level_2_all\n",
    "    for level_id in level_2_all:\n",
    "        esco_second_to_first_label[level_id] = name\n",
    "    \n",
    "esco_second_to_third_label={}\n",
    "esco_third_to_second_label = {}\n",
    "esco_third_to_first_label = {}\n",
    "for name,g in esco_clusters.groupby('label_level_2').label_level_3:#.value_counts():\n",
    "    level_3_all = sorted(g.value_counts().index.to_list())\n",
    "    esco_second_to_third_label[name] = level_3_all\n",
    "    for level_id in level_3_all:\n",
    "        esco_third_to_second_label[level_id] = name\n",
    "        esco_third_to_first_label[level_id] = esco_second_to_first_label[name]\n",
    "\n",
    "esco_first_to_third_label = {}\n",
    "for name in esco_first_to_second_label.keys():\n",
    "    level_2_all = esco_first_to_second_label[name]\n",
    "    level_3_all = []\n",
    "    for level_id in level_2_all:\n",
    "        level_3_all.append(esco_second_to_third_label[level_id])\n",
    "    esco_first_to_third_label[name] = sorted(flatten_lol(level_3_all))\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ONS vacancy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONS stock of vacancies\n",
    "raw_jvs_full, jvs_sic_letters = load_ons_vacancies()\n",
    "# Change all the columns names\n",
    "raw_jvs_full = raw_jvs_full.rename(columns = {t: jvs_sic_letters.loc[t] for t in jvs_sic_letters.index})\n",
    "\n",
    "# Select desired industries and average across time\n",
    "raw_jvs_avg = raw_jvs_full[['B', 'C', 'D_E', 'F', 'G', 'H', 'I',\n",
    "        'J', 'K', 'L_O_S', 'M_P', 'N', 'O', 'Q', 'R']].mean()\n",
    "raw_jvs_avg['A'] = np.nan\n",
    "raw_jvs_avg['T'] = np.nan\n",
    "raw_jvs_avg = raw_jvs_avg.T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute stock breakdowns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick easy math about the stock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many combinations could we expect in total?\n",
    "N_soc = len(data_df.profession_soc_code_value.value_counts())\n",
    "N_ttwa = len(data_df.ttwa11cd.value_counts())\n",
    "N_sic = len(data_df.final_sic_letter.value_counts())\n",
    "N_months = 12*3 + 10 + 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((f\"Total combinations: {N_soc}*{N_ttwa}*{N_sic}*{N_months} = \"\n",
    "       f\"{N_soc*N_ttwa*N_sic*N_months}, for {len(data_df)} job adverts\"))\n",
    "\n",
    "print((f\"Nb of combinations per month: {N_soc}*{N_ttwa}*{N_sic} = \"\n",
    "       f\"{N_soc*N_ttwa*N_sic}, for {len(data_df)/N_months:.2f} job adverts \"\n",
    "      \"on average per month\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample SOC codes\n",
    "data_df['profession_soc_code_1'] = resample_soc_to_n_digits_df(\n",
    "    data_df.profession_soc_code_value, n=1)\n",
    "\n",
    "data_df['profession_soc_code_2'] = resample_soc_to_n_digits_df(\n",
    "    data_df.profession_soc_code_value, n=2)\n",
    "\n",
    "data_df['profession_soc_code_3'] = resample_soc_to_n_digits_df(\n",
    "    data_df.profession_soc_code_value, n=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute pre-resampling stock by variuos breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible partial stock breakdowns\n",
    "timer.start_task('Get all the RAW stocks')\n",
    "raw_stocks_per_month = {}\n",
    "possible_breakdowns = [['final_sic_letter'], ['profession_soc_code_value'], ['ttwa11cd']]\n",
    "\n",
    "keys_breakdowns = ['sic_only','soc_only','ttwa_only']\n",
    "\n",
    "COMPUTE_RAW_STOCKS = True\n",
    "SAVE_RAW_STOCKS = False\n",
    "if COMPUTE_RAW_STOCKS:\n",
    "    for i,breakdown_of_interest in enumerate(possible_breakdowns):\n",
    "        raw_stock_per_month_partial, _, _ , _ = get_stock_breakdown(\n",
    "                data_df[['final_sic_letter', 'ttwa11cd','profession_soc_code_value',\n",
    "                         'vacancy_weight_new','date', 'end_date']],\n",
    "                agg_func = 'count', \n",
    "                agg_col = 'vacancy_weight_new',\n",
    "                breakdown_col = breakdown_of_interest)\n",
    "        raw_stocks_per_month[keys_breakdowns[i]] = raw_stock_per_month_partial\n",
    "        timer.end_task()\n",
    "    if SAVE_RAW_STOCKS:\n",
    "        with open(f\"{DATA_PATH}/data/aux/raw_stock_per_month_partial_all_{DATE_ID}.pickle\",'wb') as f:\n",
    "            pickle.dump(raw_stocks_per_month,f)\n",
    "else:\n",
    "    with open(f\"{DATA_PATH}/data/aux/raw_stock_per_month_partial_all_{DATE_ID}.pickle\",'rb') as f:\n",
    "        raw_stocks_per_month = pickle.load(f)\n",
    "\n",
    "for k in raw_stocks_per_month.keys():\n",
    "    raw_stocks_per_month[k] = raw_stocks_per_month[k].iloc[2:]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute post sampling stock by various breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible partial stock breakdowns\n",
    "timer.start_task('Get all the stocks')\n",
    "stocks_per_month = {}\n",
    "possible_breakdowns = [['final_sic_letter'], ['profession_soc_code_value'], ['ttwa11cd'],\n",
    "                       ['profession_soc_code_1'],['profession_soc_code_2'],['profession_soc_code_3'],\n",
    "                      ['final_sic_letter','ttwa11cd'],\n",
    "                      ['ttwa11cd','profession_soc_code_value'],\n",
    "                       ['ttwa11cd','profession_soc_code_1'],\n",
    "                      ['ttwa11cd','profession_soc_code_2'],\n",
    "                      ['ttwa11cd','profession_soc_code_3']]\n",
    "                        \n",
    "keys_breakdowns = ['sic_only','soc4_only','ttwa_only','soc1_only','soc2_only','soc3_only',\n",
    "                    'sic_ttwa','ttwa_soc4','ttwa_soc1','ttwa_soc2','ttwa_soc3']\n",
    "\n",
    "COMPUTE_STOCKS = True\n",
    "SAVE_STOCKS = False\n",
    "if COMPUTE_STOCKS:\n",
    "    for i,breakdown_of_interest in enumerate(possible_breakdowns):\n",
    "        stock_per_month_partial, _, _ , _ = get_stock_breakdown(\n",
    "                data_df[['final_sic_letter', 'ttwa11cd','region_label',\n",
    "                         'profession_soc_code_value','profession_soc_code_1',\n",
    "                         'profession_soc_code_2','profession_soc_code_3',\n",
    "                         'vacancy_weight_new','date', 'end_date']],\n",
    "                agg_func = 'sum', \n",
    "                agg_col = 'vacancy_weight_new', \n",
    "                breakdown_col = breakdown_of_interest)\n",
    "        stocks_per_month[keys_breakdowns[i]] = stock_per_month_partial\n",
    "        timer.end_task()\n",
    "    if SAVE_STOCKS:\n",
    "        with open(f\"{DATA_PATH}/data/aux/stock_per_month_partial_all_{DATE_ID}.pickle\",'wb') as f:\n",
    "            pickle.dump(stocks_per_month,f)\n",
    "else:\n",
    "    with open(f\"{DATA_PATH}/data/aux/stock_per_month_partial_all_{DATE_ID}.pickle\",'rb') as f:\n",
    "        stocks_per_month = pickle.load(f)\n",
    "\n",
    "\n",
    "## remove first two months\n",
    "for k in stocks_per_month.keys():\n",
    "    stocks_per_month[k] = stocks_per_month[k].iloc[2:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of estimates of the stock of skill demand\n",
    "\n",
    "1. Representativeness of job adverts by industry: a) what are the biggest differences before and after? b) Stock over time for Agriculture and Households\n",
    "2. Stock of skill demand by occupation\n",
    "3. Stock of skill demand by location. Then regional variations in skill demand by industry.\n",
    "4. Stock of skill demand by location. Then regional variations in skill demand by industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock of vacancies by industry: representativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the stock before and after adjustment by ONS vacancies\n",
    "sic_codes_to_use = [col for col in stocks_per_month['sic_only'].columns if 'uncertain' not in col]\n",
    "\n",
    "sic_after = stocks_per_month['sic_only'][\n",
    "    sic_codes_to_use].mean()\n",
    "\n",
    "sic_before = raw_stocks_per_month['sic_only'][sic_codes_to_use].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join everything in one dataframe, together with ONS vacancies\n",
    "full_sic_df = pd.DataFrame(sic_before).merge(pd.DataFrame(sic_after), right_index = True, left_index= True).merge(\n",
    "    pd.DataFrame(raw_jvs_avg.iloc[2:]), right_index= True, left_index= True)\n",
    "\n",
    "full_sic_df = full_sic_df.rename(columns = {'0_x': 'Pre-resampling stock',\n",
    "                                            '0_y': 'Post-resampling stock',\n",
    "                                            0: 'ONS stock'})\n",
    "full_sic_df = full_sic_df/full_sic_df.sum(axis=0)*100\n",
    "full_sic_df = full_sic_df.sort_values(by= 'Post-resampling stock')\n",
    "\n",
    "# Plot stock of vacancies before and after, together with ONS vacancies\n",
    "with sns.plotting_context('talk'):\n",
    "    f = full_sic_df[['Pre-resampling stock','ONS stock','Post-resampling stock']].rename(\n",
    "        index = lambda x: sic_letter_to_text_abbr[x]).plot(kind='barh', figsize = (16,8),\n",
    "        width = .8, color = [nesta_colours[t] for t in [1,5,2]])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Industry sector')\n",
    "    plt.xlabel('Stock of vacancies (%)')\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(f\"{output_folder}/stock_distribution_by_sector_vs_ons.svg\")\n",
    "        plt.savefig(f\"{output_folder}/stock_distribution_by_sector_vs_ons.jpg\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot percentages of stock by industry, in increasing order\n",
    "full_sic_df.rename(sic_letter_to_text_abbr)['Post-resampling stock']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the stock of vacancies before and after adjustment, then print as a DataFrame\n",
    "sic_both = pd.DataFrame(zip(sic_before.values,sic_after.values), columns =['before','after'],\n",
    "                        index = sic_before.index.map(lambda x: sic_letter_to_text_abbr[x]))\n",
    "sic_both = sic_both.sort_values('before', ascending=False)\n",
    "sic_both['after'] =sic_both['after'].astype('int')\n",
    "sic_both['before'] =sic_both['before'].astype('int')\n",
    "\n",
    "sic_quotient = []\n",
    "total_after= sic_both['after'].sum()\n",
    "total_before = sic_both['before'].sum()\n",
    "for sic in sic_both.index:\n",
    "    sic_row= sic_both.loc[sic]\n",
    "    #print(sic,sic_row)\n",
    "    weight_after = sic_row['after']/(total_after)\n",
    "    weight_before = sic_row['before']/(total_before)\n",
    "    #print(sic,weight_before,weight_after)\n",
    "    sic_quotient.append(weight_before/weight_after)\n",
    "    \n",
    "sic_both = sic_both.assign(sic_quotient = sic_quotient)\n",
    "sic_both.after = np.around(sic_both.after/sic_both.after.sum()*100,2)\n",
    "sic_both.sic_quotient = sic_both.sic_quotient.map(lambda x: np.around(x,2))\n",
    "sic_both.index.name = 'Industry sector'\n",
    "sic_both[['sic_quotient','after']].sort_values(by='sic_quotient', ascending=False).rename(\n",
    "    columns = {'sic_quotient': 'Ratio (pre-/post-adjustment stock)',\n",
    "              'after': 'Vacancies share (%)'}).iloc[:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock of vacancies by industry: Agriculture and Households\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \"missing sectors\" over time\n",
    "with sns.plotting_context('talk'):\n",
    "    fig, ax = plt.subplots(figsize = (15,6))\n",
    "    stocks_per_month['sic_only'].rename_axis('Date', axis = 0, inplace=True)\n",
    "    stocks_per_month['sic_only'][['A']].rename(columns = {'A': sic_letter_to_text['A']}).plot(\n",
    "        ax=ax, linewidth = 2, color = 'k')#nesta_colours[2])\n",
    "    ax.set_xticks([543,546,549,552,555,558,561,564,567,570,\n",
    "                   573,576,579,582,585,588,591,594,597])\n",
    "    plt.ylabel('Stock of vacancies (Agriculture)')\n",
    "    plt.ylim([700,2700])\n",
    "    stocks_per_month['sic_only'][['T']].rename(columns = {'T': 'Acitivites of households as employers'}).plot(\n",
    "        ax= ax, secondary_y = True, linewidth = 2, color = 'r')\n",
    "    plt.ylabel('Stock of vacancies (Households)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(f\"{output_folder}/stock_over_time_agriculture_households3.svg\")\n",
    "        plt.savefig(f\"{output_folder}/stock_over_time_agriculture_households3.png\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load employment by industry, for comparison\n",
    "employment_industry = pd.read_csv(\n",
    "    '/Users/stefgarasto/Local-Data/scripts/skill_demand_escoe/skill_demand/data/'+\n",
    "    'aux/employment_by_industry_15_19_all_UK.csv',\n",
    "     skiprows = 7)\n",
    "employment_industry = employment_industry[[t for t in employment_industry.columns if not 'Conf' in t]]\n",
    "employment_industry.set_index('Date')[['T13a:1 (A Agricuture & fishing (SIC 2007) : All people )',\n",
    "                                         'T13a:25 (R-U Other services (SIC 2007) : All people )']]\n",
    "employment_industry = employment_industry.rename(columns = {\n",
    "    'T13a:25 (R-U Other services (SIC 2007) : All people )': 'R_S_T_U',\n",
    "    'T13a:1 (A Agricuture & fishing (SIC 2007) : All people )': 'A',\n",
    "    'T13a:4 (B,D,E Energy & water (SIC 2007) : All people )': 'B_D_E',\n",
    "    'T13a:7 (C Manufacturing (SIC 2007) : All people )': 'C',\n",
    "    'T13a:10 (F Construction (SIC 2007) : All people )': 'F',\n",
    "    'T13a:13 (G,I Distribution, hotels & restaurants (SIC 2007) : All people )': 'G_I',\n",
    "    'T13a:16 (H,J Transport & Communication (SIC 2007) : All people )': 'H_J',\n",
    "    'T13a:19 (K-N Banking finance & insurance etc. (SIC 2007) : All people )': 'K_M_N',\n",
    "    'T13a:22 (O-Q Public admin education & health (SIC 2007) : All people )': 'O_P_Q'\n",
    "})\n",
    "\n",
    "a = ['B_D_E','C','F','G_I','H_J','K_M_N','O_P_Q']\n",
    "print('Employment by industry')\n",
    "print(employment_industry[a].mean()/employment_industry[a].mean().sum()*100)\n",
    "\n",
    "# Join up ONS vacancy data by industries based on our custom groupings\n",
    "tmp = raw_jvs_full[['B','C','D_E','F','G','I','H','J','K','M','N','O','P','Q','R','S']].mean()\n",
    "tmp['G_I'] = tmp['G'] + tmp['I']\n",
    "tmp = tmp.drop(['G','I'])\n",
    "tmp['H_J'] = tmp['H'] + tmp['J']\n",
    "tmp = tmp.drop(['H','J'])\n",
    "tmp['B_D_E'] = tmp['B'] + tmp['D_E']\n",
    "tmp = tmp.drop(['B','D_E'])\n",
    "tmp['K_M_N'] = tmp['K'] + tmp['M'] + tmp['N']\n",
    "tmp = tmp.drop(['K','M','N'])\n",
    "tmp['O_P_Q'] = tmp['O'] + tmp['P'] + tmp['Q']\n",
    "tmp = tmp.drop(['O','P','Q'])\n",
    "tmp = tmp.drop(['R','S'])\n",
    "\n",
    "print('ONS vacancies by industries')\n",
    "print(100*tmp/tmp.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock by occupation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition of stock by occupations, averaged across time and comparison with employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ons_employment(ons_filename):\n",
    "    if ons_filename == 'employment_by_occupation1_15_19_in_UK.csv':\n",
    "        skiprows= 10#43\n",
    "        nrows = 10\n",
    "        n=1\n",
    "    elif ons_filename == 'employment_by_occupation2_15_19_in_UK.csv':\n",
    "        skiprows = 10#59\n",
    "        nrows = 26\n",
    "        n=2\n",
    "    elif ons_filename == 'employment_by_occupation3_15_19_in_UK.csv':\n",
    "        skiprows = 10#124\n",
    "        nrows = 91\n",
    "        n=3\n",
    "    elif ons_filename == 'employment_by_occupation4_15_19_in_UK.csv':\n",
    "        skiprows = 9#124\n",
    "        nrows = 370\n",
    "        n=4\n",
    "    employment_by_soc = pd.read_csv(f\"{local_data_path}/aux/{ons_filename}\",\n",
    "                                 skiprows= skiprows, nrows = nrows)\n",
    "    \n",
    "    try:\n",
    "        employment_by_soc = employment_by_soc.set_index('Occupation')\n",
    "        employment_by_soc = employment_by_soc[[t for t in employment_by_soc.columns if not 'Conf' in t]]\n",
    "        employment_by_soc = employment_by_soc.rename(columns = {t: f\"employment {t[-4:]}\" \n",
    "                                                                for t in employment_by_soc.columns})\n",
    "        employment_by_soc = employment_by_soc.iloc[1:]\n",
    "        #employment_by_soc = employment_by_soc/employment_by_soc.sum()*100\n",
    "    except:\n",
    "        print('not good')\n",
    "        printdf(employment_by_soc.head())\n",
    "    return employment_by_soc\n",
    "    \n",
    "employment_by_soc1 = load_ons_employment('employment_by_occupation1_15_19_in_UK.csv')\n",
    "employment_by_soc2 = load_ons_employment('employment_by_occupation2_15_19_in_UK.csv')\n",
    "employment_by_soc3 = load_ons_employment('employment_by_occupation3_15_19_in_UK.csv')\n",
    "employment_by_soc4 = load_ons_employment('employment_by_occupation4_15_19_in_UK.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# composition by 1-digit soc codes\n",
    "active_stock = deepcopy(stocks_per_month['soc1_only'].resample('Y').mean().T)\n",
    "# normalise each year to get shares\n",
    "print(100*active_stock.mean(axis=1).values/employment_by_soc1.mean(axis=1).values)\n",
    "active_stock = active_stock/active_stock.sum()*100\n",
    "active_stock = active_stock.rename(columns = \n",
    "                                   {pd.to_datetime(f\"{t}-12-31\"): t \n",
    "                                    for t in ['2015','2016','2017','2018','2019']})\n",
    "active_stock['Occupation'] = active_stock.index.map(lambda x: socnames_dict[x])\n",
    "active_stock['Avg share (adverts)'] = active_stock[\n",
    "    [t for t in active_stock.columns if not 'Occupation' in t]].mean(axis=1)\n",
    "tmp_ons = deepcopy(employment_by_soc1)\n",
    "tmp_ons= np.around(tmp_ons/tmp_ons.sum()*100,2)\n",
    "tmp_ons['Avg share (ONS)'] = tmp_ons.mean(axis=1).values\n",
    "tmp_ons['soc_code'] = tmp_ons.index.map(lambda x: float(x[:1]))\n",
    "\n",
    "active_stock = active_stock.merge(tmp_ons[['soc_code','Avg share (ONS)']], left_index= True, right_on = 'soc_code')\n",
    "\n",
    "printdf(active_stock[['Occupation','Avg share (adverts)','Avg share (ONS)']].head(20))\n",
    "\n",
    "# Print in a format that can easily copy-pasted to Excel to make tables\n",
    "for i in ['Occupation','Avg share (adverts)','Avg share (ONS)']:\n",
    "    for t in active_stock[i].values:\n",
    "        if i=='Occupation':\n",
    "            print(t)\n",
    "        else:\n",
    "            print(f\"{t:.2f}\")\n",
    "    print('---')\n",
    "for t in active_stock[i].index:\n",
    "        print(t[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# composition by n-digit soc codes\n",
    "n= 4\n",
    "active_stock = deepcopy(stocks_per_month[f'soc{n}_only'].resample('Y').mean().T)\n",
    "# normalise each year to get shares\n",
    "if n==1:\n",
    "    employment_of_interest = employment_by_soc1\n",
    "elif n==2:\n",
    "    employment_of_interest = employment_by_soc2\n",
    "elif n ==3:\n",
    "    employment_of_interest = employment_by_soc3\n",
    "elif n==4:\n",
    "    employment_of_interest = employment_by_soc4\n",
    "else:\n",
    "    stop\n",
    "\n",
    "#print(100*active_stock.mean(axis=1).values/employment_of_interest.mean(axis=1).values)\n",
    "active_stock = active_stock/active_stock.sum()*100\n",
    "active_stock = active_stock.rename(columns = \n",
    "                                   {pd.to_datetime(f\"{t}-12-31\"): t for t in ['2015','2016','2017','2018','2019']})\n",
    "active_stock['Occupation'] = active_stock.index.map(lambda x: socnames_dict[x])\n",
    "active_stock['Avg share (adverts)'] = active_stock[\n",
    "    [t for t in active_stock.columns if not 'Occupation' in t]].mean(axis=1)\n",
    "tmp_ons = deepcopy(employment_of_interest)\n",
    "tmp_ons= np.around(tmp_ons/tmp_ons.sum()*100,2)\n",
    "tmp_ons['Avg share (ONS)'] = tmp_ons.mean(axis=1).values\n",
    "tmp_ons['soc_code'] = tmp_ons.index.map(lambda x: float(x[:n]))\n",
    "\n",
    "active_stock = active_stock.merge(tmp_ons[['soc_code','Avg share (ONS)']], left_index= True, right_on = 'soc_code') \n",
    "printdf(active_stock[['Occupation','Avg share (adverts)','Avg share (ONS)']].head(20))\n",
    "\n",
    "# Print in a format that can easily copy-pasted to Excel to make tables\n",
    "for i in ['Occupation','Avg share (adverts)','Avg share (ONS)']:\n",
    "    for t in active_stock[i].values:\n",
    "        if i=='Occupation':\n",
    "            print(t)\n",
    "        else:\n",
    "            print(f\"{t:.2f}\")\n",
    "    print('---')\n",
    "for t in active_stock[i].index:\n",
    "        print(t[:n])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOC over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which occupations have changed the most over time?\n",
    "tmp = stocks_per_month['soc1_only'].resample('Y').mean().T\n",
    "tmp = tmp/tmp.sum()*100\n",
    "tmp = tmp[tmp.mean(axis=1)>.1]\n",
    "tmp = tmp.assign(ratio_2019_2015 = 100*(tmp['2019-12-31']-tmp['2015-12-31'])/tmp['2015-12-31']).fillna(0)\n",
    "tmp.index= tmp.index.map(lambda x: socnames_dict[x])\n",
    "\n",
    "tmp = tmp.rename(columns = {pd.to_datetime(f'{t}-12-31'): f\"{t} (%)\" for t in ['2019','2018','2017','2016','2015']})\n",
    "tmp = tmp.rename(columns = {'ratio_2019_2015': 'relative change 2015 to 2019 (%)'})\n",
    "tmp.name = 'Occupation (1-digit SOC)'\n",
    "\n",
    "printdf(tmp.applymap(lambda x: np.around(x,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Breakdown of more granular occupations (share of vacancies)\n",
    "stock_by_soc3 = stocks_per_month['soc3_only'].resample('Y').mean().T\n",
    "# turn into share\n",
    "stock_by_soc3 = stock_by_soc3/stock_by_soc3.sum()*100\n",
    "# Only keep occupations with, on average, at least .1% share\n",
    "stock_by_soc3 = stock_by_soc3[stock_by_soc3.mean(axis=1)>.1]\n",
    "stock_by_soc3 = stock_by_soc3.assign(ratio_2019_2015 = 100*(stock_by_soc3['2019-12-31']-stock_by_soc3['2015-12-31']\n",
    "                                       )/stock_by_soc3['2015-12-31']).fillna(0)\n",
    "\n",
    "stock_by_soc3['profession_soc_code_1'] = stock_by_soc3.index.map(lambda x: resample_soc_to_n_digits(x,n=2))\n",
    "\n",
    "# group by 1-digit SOC\n",
    "soc1_groups = stock_by_soc3.groupby('profession_soc_code_1')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot minor groups grouped by Major groups - changes over time\n",
    "for g in range(1,10):\n",
    "    with sns.plotting_context('talk'):\n",
    "        group1_2015 = soc1_groups.get_group(g).sort_values(\n",
    "            pd.to_datetime('2015-12-31'), ascending=False)[pd.to_datetime('2015-12-31')]\n",
    "        group1_2019 = soc1_groups.get_group(g).sort_values(\n",
    "            pd.to_datetime('2015-12-31'), ascending=False)[pd.to_datetime('2019-12-31')]\n",
    "        group1_max = group1_2015.sum()\n",
    "        group1_2019_norm = group1_2019\n",
    "        group1_2015_norm = group1_2015\n",
    "        pd.DataFrame(zip(group1_2019_norm, group1_2015_norm), columns = ['2019','2015'],\n",
    "                    index = group1_2015_norm.index.map(lambda x: socnames_dict[x])).T.plot(\n",
    "            kind='barh', stacked=True, figsize = (15,5), color = aug_nesta_colours)#, ax=f.gca())\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "        plt.title(socnames_dict[g])\n",
    "        plt.xlabel('Vacancies share (%)')\n",
    "        plt.ylabel('Year')\n",
    "        plt.tight_layout()\n",
    "        if SAVEFIG:\n",
    "            plt.savefig(f\"{output_folder}/4d_breakdown_share_change_group{g}.svg\")\n",
    "            plt.savefig(f\"{output_folder}/4d_breakdown_share_change_group{g}.png\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soc1_groups.get_group(6).ratio_2019_2015.mean())\n",
    "soc1_groups.get_group(6).sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock of vacancies across TTWA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust by economically active residents and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot stock distribution by TTWA averaged across time\n",
    "plt.close('all')\n",
    "year = '2018'\n",
    "row = {'2016': 1, '2017': 2, '2018': 3, '2019': 4}\n",
    "# normalise by employment for each year then take the average across years\n",
    "data_prep = stocks_per_month['ttwa_only'].resample('Y').mean().T\n",
    "\n",
    "# join with relevant employment estimates\n",
    "data_prep = data_prep.merge(employment_levels, left_on='ttwa11cd', right_index=True)\n",
    "\n",
    "for year in ['2015','2016','2017','2018','2019']:\n",
    "    year_index = pd.to_datetime(f\"31-12-{year}\")\n",
    "    data_prep[year_index] = data_prep[year_index]/data_prep[f\"employment_{year}\"]*100\n",
    "    data_prep = data_prep.rename(columns = {year_index: f'stock_{year}'})\n",
    "data_prep['Stock'] = data_prep[[t for t in data_prep.columns if 'stock' in t]].mean(axis=1)\n",
    "data_prep['employment_avg'] = data_prep[[t for t in data_prep.columns if 'employment' in t]].mean(axis=1)\n",
    "\n",
    "data_prep = data_prep.reset_index()\n",
    "data_prep = data_prep.fillna(0)\n",
    "\n",
    "#Don't show TTWAs with less than 40,000 in employment\n",
    "data_prep = data_prep[data_prep.ttwa11cd.map(lambda x:\n",
    "                            x in biggest_employment_ttwas40)] \n",
    "\n",
    "# Plot map\n",
    "w=8\n",
    "fig,ax = plt.subplots(figsize = (w,14))\n",
    "shp_folder = '/path/to/maps/files'\n",
    "gb_filename = os.path.join(shp_folder,\n",
    "                'Countries2016/Countries_December_2016_Ultra_Generalised_Clipped_Boundaries_in_Great_Britain.shp')\n",
    "ni_filename = os.path.join(shp_folder,\n",
    "                'OSNI_Open_Data_Largescale_Boundaries__Country_2016.shp')\n",
    "shp_filename = os.path.join(shp_folder,\n",
    "    'TTWA2011_full_clipped/Travel_to_Work_Areas_December_2011_Full_Clipped_Boundaries_in_United_Kingdom.shp')\n",
    "# Plot\n",
    "data_to_plot= pd.DataFrame(data_prep[['Stock','ttwa11cd']])\n",
    "col_to_plot = 'Stock'\n",
    "with sns.plotting_context('talk'):\n",
    "    _, _, xs, ys = draw_map(data_to_plot, col_to_plot, 'GnBu', \n",
    "                        data_to_plot[col_to_plot].min(), \n",
    "                        data_to_plot[col_to_plot].max(), \n",
    "                        gb_filename, ni_filename, #gb_filename\n",
    "                        shp_filename, roi_col = 'ttwa11cd',\n",
    "                        shp_col = 'ttwa11cd',\n",
    "                        fig = fig, ax= ax)\n",
    "\n",
    "tmp = ax.set_title('Stock of vacancies per 100 economically active residents', fontsize = 18)\n",
    "\n",
    "if SAVEFIG:\n",
    "    plt.savefig(f\"{output_folder}/avg_stock_by_100_employment_by_ttwa.svg\", bbox_inches = 'tight')\n",
    "    plt.savefig(f\"{output_folder}/avg_stock_by_100_employment_by_ttwa.png\", bbox_inches = 'tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top and bottom TTWAs for vacancies per 100 people in employment\n",
    "plt.close('all')\n",
    "data_prep['ttwa11nm'] = data_prep['ttwa11cd'].map(lambda x: ttwa_code2name[x])\n",
    "print('Bottom 5')\n",
    "print(data_prep.sort_values('Stock', ascending = True).head(10).ttwa11nm.tolist())\n",
    "printdf(data_prep.sort_values('Stock', ascending = True).head())\n",
    "print('Top 5')\n",
    "print(data_prep.sort_values('Stock', ascending = False).head(10).ttwa11nm.tolist())\n",
    "printdf(data_prep.sort_values('Stock', ascending = False).head())\n",
    "print(np.percentile(data_prep.Stock,1),np.percentile(data_prep.Stock,5),\n",
    "     np.percentile(data_prep.Stock,95),np.percentile(data_prep.Stock,99))\n",
    "print(data_prep.Stock.min(),data_prep.Stock.max())\n",
    "print(np.percentile(data_prep.Stock,99)/np.percentile(data_prep.Stock,1))\n",
    "print(data_prep.Stock.max()/data_prep.Stock.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock by SIC x TTWA: which SIC codes are more evenly distributed across TTWA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "data_to_use = deepcopy(stocks_per_month['sic_ttwa'])\n",
    "data_prep = data_to_use.mean()\n",
    "data_prep = data_prep\n",
    "data_prep = data_prep.reset_index(level=1).rename(columns = {0:'stock'})\n",
    "data_prep = data_prep.reset_index(drop=False)\n",
    "data_prep = data_prep[data_prep.final_sic_letter!='uncertain']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange stock by sic and ttwa from long to wide form\n",
    "tmp = []\n",
    "for sic in data_prep.final_sic_letter.value_counts().index:\n",
    "    tmp.append(data_prep[data_prep.final_sic_letter==sic].stock.values)\n",
    "all_ttwas = data_prep[data_prep.final_sic_letter=='A'].ttwa11cd\n",
    "\n",
    "sic_by_ttwa = pd.DataFrame(tmp, columns = all_ttwas, \n",
    "             index= data_prep.final_sic_letter.value_counts().index)\n",
    "\n",
    "# drop the part of stock that is not actually matched to a TTWA\n",
    "sic_by_ttwa = sic_by_ttwa.drop(columns = ['not_found'])\n",
    "sic_by_ttwa.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute proportion of skill demand by sector across all UK\n",
    "sic_value_counts = data_df.final_sic_letter.value_counts()\n",
    "full_sector_in_uk = sic_by_ttwa.sum(axis=1)/sic_by_ttwa.sum().sum()\n",
    "quotient_sic_ttwa = pd.DataFrame(columns = sic_by_ttwa.columns,\n",
    "                                index = sic_by_ttwa.index)\n",
    "\n",
    "# get the sector location quotients for all TTWAs. Note, this is computed using the WHOLE of the UK\n",
    "for chosen_ttwa in sic_by_ttwa.columns:\n",
    "    quotient_sic_ttwa[chosen_ttwa] = (sic_by_ttwa[chosen_ttwa]/sic_by_ttwa[chosen_ttwa].sum())/full_sector_in_uk\n",
    "\n",
    "# only keep largest TTWAs (economically active residents >40K)\n",
    "quotient_sic_ttwa = quotient_sic_ttwa[biggest_employment_ttwas40]\n",
    "\n",
    "# now compute the Gini coefficient for each sector. This is computed ONLY on SELECTED TTWAs\n",
    "gini_by_sector = []\n",
    "for chosen_sic in sic_by_ttwa.index:\n",
    "    gini_by_sector.append(gini(quotient_sic_ttwa.loc[chosen_sic].to_numpy()))\n",
    "\n",
    "quotient_sic_ttwa= quotient_sic_ttwa.assign(gini_index = gini_by_sector)\n",
    "\n",
    "quotient_sic_ttwa = quotient_sic_ttwa.assign(counts = sic_value_counts)\n",
    "\n",
    "quotient_sic_ttwa.index = quotient_sic_ttwa.index.map(lambda x: sic_letter_to_text_abbr[x])\n",
    "\n",
    "# show the London location quotients, from high to low\n",
    "printdf(pd.DataFrame(quotient_sic_ttwa[london_code]).rename(\n",
    "    columns = {'E30000234': 'London local quotient'}).sort_values(by = 'London local quotient', ascending= False))\n",
    "\n",
    "# Add Cambridge location quotients\n",
    "ttwaname = 'Cambridge'\n",
    "printdf(pd.DataFrame(quotient_sic_ttwa[ttwa_name2code[ttwaname]]).rename(\n",
    "    columns = {ttwa_name2code[ttwaname]: f'{ttwaname} local quotient'}).sort_values(\n",
    "    by = f'{ttwaname} local quotient', ascending= False))\n",
    "\n",
    "\n",
    "# Show Gini coefficients, high to low + ratio\n",
    "rho_gini_counts= np.corrcoef(\n",
    "    quotient_sic_ttwa[['gini_index','counts']].astype('float32').to_numpy().T)[0,1]\n",
    "print((f\"Correlation between gini index and sic value counts: \"\n",
    "       f\"{rho_gini_counts:.2f}\"))\n",
    "print((f\"Highest Gini index is {quotient_sic_ttwa.gini_index.sort_values().iloc[-2]:.2f}\"))\n",
    "print((f\"Lowest Gini index is {quotient_sic_ttwa.gini_index.min():.2f}\"))\n",
    "print((f\"Ratio between highest and lowest Gini index is \"\n",
    "      f\"{quotient_sic_ttwa.gini_index.sort_values().iloc[-2]/quotient_sic_ttwa.gini_index.min():.2f}\"))\n",
    "\n",
    "pd.DataFrame(quotient_sic_ttwa['gini_index'].sort_values(\n",
    "          ascending = False).rename('Gini index')).iloc[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot location quotients for Information and communication and Administrative and support service activities\n",
    "sic_letter2use = 'Information and communication'\n",
    "sic_letter2use2 = 'Administrative and support service activities'\n",
    "\n",
    "# plot best and worst sector next to each other in order\n",
    "quotient_sic_ttwa_red = quotient_sic_ttwa.drop(['gini_index','counts'],axis= 1).T[[sic_letter2use,sic_letter2use2]]\n",
    "\n",
    "# only keep the ones with > 250k economically active residents\n",
    "quotient_sic_ttwa_red = quotient_sic_ttwa_red[\n",
    "    quotient_sic_ttwa_red.index.map(lambda x: x in biggest_employment_ttwas250)]\n",
    "with sns.plotting_context('talk'):\n",
    "    quotient_sic_ttwa_red.sort_values(by = sic_letter2use).rename(lambda x: ttwa_code2name[x]).plot(\n",
    "        kind = 'barh', figsize = (15,11), color = [nesta_colours[t] for t in [1,2]])\n",
    "    plt.legend(loc='lower right', title= 'Industry')\n",
    "    plt.plot([quotient_sic_ttwa_red['Information and communication'].min(),\n",
    "              quotient_sic_ttwa_red['Information and communication'].min()],\n",
    "            [-1,32], '--', color = nesta_colours[1])\n",
    "    plt.plot([quotient_sic_ttwa_red['Information and communication'].max(),\n",
    "              quotient_sic_ttwa_red['Information and communication'].max()],\n",
    "            [-1,32], '--', color = nesta_colours[1])\n",
    "    plt.ylabel('TTWA')\n",
    "    plt.xlabel('Location quotient')\n",
    "    plt.tight_layout()\n",
    "\n",
    "print(quotient_sic_ttwa_red[sic_letter2use].max()/quotient_sic_ttwa_red[sic_letter2use].min())\n",
    "print(quotient_sic_ttwa_red[sic_letter2use2].max()/quotient_sic_ttwa_red[sic_letter2use2].min())\n",
    "\n",
    "if SAVEFIG:\n",
    "    plt.savefig(f\"{output_folder}/ttwa_quotients_IT_Admin.svg\")\n",
    "    plt.savefig(f\"{output_folder}/ttwa_quotients_IT_Admin.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Top TTWAs for selected industries\n",
    "quotient_sic_ttwa_red = quotient_sic_ttwa.drop(['gini_index','counts'],axis= 1).T\n",
    "# Top TTWAs in Finance:\n",
    "print('Top TTWAs in Finance')\n",
    "print(quotient_sic_ttwa_red['Financial and insurance activities'].sort_values(ascending=False).head(10).rename(\n",
    "    lambda x: ttwa_code2name[x]).index.tolist())\n",
    "print('Top TTWAs in Agriculture')\n",
    "print(quotient_sic_ttwa_red['Agriculture, forestry and fishing'].sort_values(ascending=False).head(10).rename(\n",
    "    lambda x: ttwa_code2name[x]).index.tolist())\n",
    "print('Top TTWAs in Utilities')\n",
    "print(quotient_sic_ttwa_red['Utilities (energy, water and waste)'].sort_values(ascending=False).head(10).rename(\n",
    "    lambda x: ttwa_code2name[x]).index.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock of vacancies by TTWA and skill clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute stock by skill cluster over the years using the crosswalk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosswalk from stock by occupation only (soc_by_clusters has been loaded earlier)\n",
    "month_by_clusters = {}\n",
    "for k_esco in ['level_1','level_2','level_3']:\n",
    "    k = f'4d_by_{k_esco}'\n",
    "    active_crosswalk = {}\n",
    "    for year in soc_by_clusters.keys():\n",
    "        active_crosswalk[year] = deepcopy(soc_by_clusters_norm[year][k].fillna(0))\n",
    "\n",
    "    # crosswalk from stock by soc to stock by cluster\n",
    "    month_by_cluster = pd.DataFrame(index= stocks_per_month['soc4_only'].index,\n",
    "                                   columns = active_crosswalk[year].index)\n",
    "    for month in stocks_per_month['soc4_only'].index:\n",
    "        row = deepcopy(stocks_per_month['soc4_only'].loc[month,:]).to_numpy()\n",
    "        row = row[:,np.newaxis]\n",
    "        year_to_use = str(month.year)\n",
    "        row = np.dot(active_crosswalk[year_to_use].fillna(0).to_numpy(),row)\n",
    "        month_by_cluster.loc[month,:] = row[:,0]\n",
    "    \n",
    "    # check the total stock is roughly the same\n",
    "    print(month_by_cluster.sum().sum()//1000, stocks_per_month['soc4_only'].sum().sum()//1000)\n",
    "    \n",
    "    month_by_clusters[k_esco] = month_by_cluster.astype('float')\n",
    "\n",
    "SAVE_CLUSTER_STOCK = False\n",
    "if SAVE_CLUSTER_STOCK:\n",
    "    with gzip.GzipFile(f\"{DATA_PATH}/data/aux/stock_by_month_by_cluster_all_{DATE_ID}.gz\", 'wb') as f:\n",
    "        pickle.dump(month_by_clusters,f)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock for cluster level 1\n",
    "stock_composition_1 = pd.DataFrame(month_by_clusters['level_1'].resample('Y').mean())\n",
    "stock_composition_1.index = stock_composition_1.index.map(lambda x: str(x)[:4])\n",
    "stock_composition_1 = stock_composition_1.T\n",
    "\n",
    "level_1_composition = stock_composition_1/stock_composition_1.sum(axis=0)*100\n",
    "level_1_composition = level_1_composition.drop(15) #iloc[:201]\n",
    "level_1_composition.index = level_1_composition.index.map(lambda x: \n",
    "                                                    cluster_labels_1.loc[int(x)].label)\n",
    "level_1_composition.index= level_1_composition.index.map(lambda x: x.capitalize())\n",
    "\n",
    "level_1_composition = level_1_composition.sort_values('2015', ascending = False)\n",
    "print(100*(level_1_composition['2019'] - level_1_composition['2015']) / level_1_composition['2015'])\n",
    "\n",
    "# Define the colours for each cluster at level 1\n",
    "colors_for_cluster_level_1 = {}\n",
    "for ix,t in enumerate(level_1_composition.index):\n",
    "    print(t)\n",
    "    colors_for_cluster_level_1[t]= ix\n",
    "for t in level_1_composition.mean(axis=1)/level_1_composition.mean(axis=1).sum():\n",
    "    print(f\"{100*t:.2f}\")\n",
    "\n",
    "# Plot the distribution over years\n",
    "with sns.plotting_context('talk'):\n",
    "    f = plt.figure()\n",
    "    level_1_composition[['2019','2018','2017','2016','2015']].T.plot(\n",
    "        kind='barh', stacked=True, figsize = (14,5.8), ax=f.gca(), color = aug_nesta_colours)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "    plt.xlabel('Vacancies share (%)')\n",
    "    plt.ylabel('Year')\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(f\"{output_folder}/stock_by_cluster_level1_2015_2019.png\")\n",
    "        plt.savefig(f\"{output_folder}/stock_by_cluster_level1_2015_2019.svg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_YEAR = '2015'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stock for cluster level 2\n",
    "stock_composition_2 = pd.DataFrame(month_by_clusters['level_2'].resample('Y').mean())\n",
    "stock_composition_2.index = stock_composition_2.index.map(lambda x: str(x)[:4])\n",
    "stock_composition_2 = stock_composition_2.T\n",
    "\n",
    "level_2_composition = stock_composition_2/stock_composition_2.sum(axis=0)*100\n",
    "level_2_composition = level_2_composition.drop(76) #iloc[:201]\n",
    "level_2_composition.index = level_2_composition.index.map(lambda x: \n",
    "                                                    cluster_labels_2.loc[int(x)].label)\n",
    "\n",
    "level_2_composition.index = level_2_composition.index.map(lambda x: x.capitalize())\n",
    "\n",
    "level_2_composition['mean_value'] = level_2_composition.mean(axis=1)\n",
    "level_2_composition = level_2_composition.sort_values('mean_value', ascending = False)\n",
    "\n",
    "larger_rows_2 = (level_2_composition.mean(axis=1)>0.9) #| (level_2_composition.index == 'emergency healthcare')\n",
    "\n",
    "level_2_composition = level_2_composition.drop(columns = 'mean_value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the composition by cluster level 2 (coloured by cluster level 1)\n",
    "f = plt.figure(figsize = (11,12))\n",
    "tmp = deepcopy(level_2_composition.loc[larger_rows_2,['2015','2016','2017','2018','2019']].reset_index())\n",
    "tmp = tmp.melt(id_vars = 'row_0', value_vars = ['2015','2016','2017','2018','2019'], var_name= 'Year')\n",
    "\n",
    "with sns.plotting_context('talk'):\n",
    "    sns.barplot(x = 'value', y = 'row_0', hue= 'Year',\n",
    "            data= tmp, ax = f.gca(), palette = sns.color_palette([nesta_colours[t] for t in [6,4,10,8,7]]))\n",
    "    \n",
    "    plt.ylabel('Second level skills clusters')\n",
    "    plt.xlabel('Vacancies share (%)')\n",
    "    plt.tight_layout()\n",
    "    for t in f.gca().get_yticklabels():\n",
    "        level_up = esco_second_to_first_label[t.get_text().lower()]\n",
    "        color_id = colors_for_cluster_level_1[level_up.capitalize()]\n",
    "        t.set_color(nesta_colours[color_id])\n",
    "        #f.gca().set_yticklabels(t)#print(level_up,color_id)\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(f\"{output_folder}/stock_by_cluster_level2_2015_2019.png\")\n",
    "        plt.savefig(f\"{output_folder}/stock_by_cluster_level2_2015_2019.svg\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# top decreasing and increasing clusters among the ones with a least 1% share\n",
    "# increase and decrease is taken as the average change across consecutive years or ...\n",
    "# ...as the change between the average of consecutive years\n",
    "active_composition = deepcopy(level_2_composition[larger_rows_2])\n",
    "tmp = active_composition\n",
    "\n",
    "for base_year in ['2015','2016','2017','2018']:\n",
    "    next_year = f\"{int(base_year)+1}\"\n",
    "    tmp[f'relative_change_{base_year}'] = (active_composition[next_year] - active_composition[base_year]\n",
    "                                 )/active_composition[base_year]\n",
    "tmp['average_2015_2016']= (tmp['2015'] + tmp['2016'])/2\n",
    "tmp['average_2018_2019']= (tmp['2018'] + tmp['2019'])/2\n",
    "tmp['relative_change_1516_1819'] = (tmp['average_2018_2019'] - tmp['average_2015_2016'])/tmp['average_2015_2016']\n",
    "tmp['relative_change_avg'] = tmp[[f'relative_change_{t}' for t in ['2015','2016','2017','2018']]].mean(axis=1)\n",
    "tmp['relative_change_std'] = tmp[[f'relative_change_{t}' for t in ['2015','2016','2017','2018']]].std(axis=1)\n",
    "\n",
    "top5_relative_changes = tmp.sort_values('relative_change_avg').tail(5).index.to_list()\n",
    "bottom5_relative_changes = tmp.sort_values('relative_change_avg').head(5).index.to_list()\n",
    "top5_erratic_changes = tmp.sort_values('relative_change_std').tail(5).index.to_list()\n",
    "\n",
    "100*tmp.sort_values(by = 'relative_change_avg', ascending= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composition of stock by cluster level 3\n",
    "stock_composition_3 = pd.DataFrame(month_by_clusters['level_3'].resample('Y').mean())\n",
    "stock_composition_3.index = stock_composition_3.index.map(lambda x: str(x)[:4])\n",
    "stock_composition_3 = stock_composition_3.T\n",
    "\n",
    "\n",
    "level_3_composition = stock_composition_3/stock_composition_3.sum(axis=0)\n",
    "level_3_composition = level_3_composition.drop(201) #iloc[:201]\n",
    "level_3_composition.index = level_3_composition.index.map(lambda x: \n",
    "                                                    cluster_labels_3.loc[int(x)].label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters at level 3 with largest share of demand\n",
    "larger_rows_3 = (level_3_composition.mean(axis=1)>0.3/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show changes for some example clusters\n",
    "for t in ['digital marketing', 'marketing', 'marketing (branding)', 'product management']:\n",
    "    decrease_2015 = (level_3_composition.loc[t]['2019'] - level_3_composition.loc[t]['2015']\n",
    "                    )/level_3_composition.loc[t]['2015']\n",
    "    decrease_2016 = (level_3_composition.loc[t]['2019'] - level_3_composition.loc[t]['2016']\n",
    "                    )/level_3_composition.loc[t]['2016']\n",
    "    print(t, decrease_2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skillset_to_check = list(set(top5_relative_changes + bottom5_relative_changes + top5_erratic_changes))\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# show changes for more example clusters\n",
    "for skillset in cluster_labels_2.label.tolist():\n",
    "    if skillset == 'nan':\n",
    "        continue\n",
    "    sub_skillset = []\n",
    "    for t in esco_second_to_third_label[skillset]:\n",
    "        if t in level_3_composition.index:\n",
    "            sub_skillset.append(t)\n",
    "\n",
    "    select_level_3 = 100*level_3_composition.loc[sub_skillset].T\n",
    "    if skillset== 'marketing & product management':\n",
    "        select_level_3.columns = ['digital marketing','marketing','marketing (branding)',\n",
    "                                                    'product management']\n",
    "    max_tmp = select_level_3.loc['2015'].sum()\n",
    "    if max_tmp<0.1:\n",
    "        continue\n",
    "    #select_level_3 = select_level_3/max_tmp\n",
    "    with sns.plotting_context('talk'):\n",
    "        select_level_3.sort_index(ascending=False).plot(kind='barh', stacked=True, figsize = (13,5), \n",
    "                                             color = aug_nesta_colours)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "        plt.title(f\"{skillset.capitalize()}\")\n",
    "        plt.xlabel('Vacancies share (%)')\n",
    "        plt.ylabel('Year')\n",
    "        plt.tight_layout()\n",
    "        if SAVEFIG:\n",
    "            plt.savefig(f\"{output_folder}/stock_by_cluster_{skillset}_2015_2019.png\")\n",
    "            plt.savefig(f\"{output_folder}/stock_by_cluster_{skillset}_2015_2019.svg\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and analyse diversity of skill clusters across TTWAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosswalk from (stock by ttwa x soc) to (stock by ttwa x skill cluster)\n",
    "''' For each month take the distribution by ttwa and soc, multiply the distribution of stock\n",
    "by soc within each ttwa by the crosswalk soc x clusters, then sum across the soc.\n",
    "The result is the stock by ttwa and cluster for each month'''\n",
    "\n",
    "timer.start_task()\n",
    "\n",
    "month_by_ttwa_by_clusters = {}\n",
    "month_by_ttwa_by_clusters_df = {}\n",
    "which_soc = 3\n",
    "for k_esco in ['level_3','level_2','level_1']:\n",
    "    print(k_esco)\n",
    "    k = f'{which_soc}d_by_{k_esco}'\n",
    "    active_crosswalk = {}\n",
    "    for year in soc_by_clusters.keys():\n",
    "        active_crosswalk[year] = deepcopy(soc_by_clusters_norm[year][k].fillna(0)).T\n",
    "        \n",
    "    month_by_ttwa_by_cluster = {}\n",
    "    month_by_ttwa_by_cluster_df = {}\n",
    "    for i,month in enumerate(stocks_per_month[f'ttwa_soc{which_soc}'].index):\n",
    "        #month_key\n",
    "        if i%10==9:\n",
    "            print(month)\n",
    "        year_to_use = str(month.year)\n",
    "        month_by_ttwa_by_cluster[month] = {}\n",
    "        rows = deepcopy(stocks_per_month[f'ttwa_soc{which_soc}'].loc[month].reset_index(level=0))\n",
    "        for ttwa in list(set(rows.index)):\n",
    "            if ttwa == 'not_found':\n",
    "                continue\n",
    "            row2 = rows.loc[ttwa].reset_index(drop=True)\n",
    "            #row2['profession_soc_code_3'] = row2['profession_soc_code_3'].astype('category')\n",
    "            row2 = row2.set_index(f'profession_soc_code_{which_soc}')#value')\n",
    "            row2_merged = deepcopy(row2.merge(active_crosswalk[year_to_use],\n",
    "                                                                         left_index= True, \n",
    "                                                                         right_index= True).fillna(0))\n",
    "            \n",
    "            # perform dot product\n",
    "            row_by_cluster = row2_merged.loc[:,row2_merged.columns[1:202]].mul(\n",
    "                row2_merged[month],axis=0).sum()\n",
    "            month_by_ttwa_by_cluster[month][ttwa] = row_by_cluster\n",
    "        \n",
    "        # combine into  a dataframe\n",
    "        month_by_ttwa_by_cluster_df[month] = pd.DataFrame.from_dict(\n",
    "            month_by_ttwa_by_cluster[month]).stack()\n",
    "        \n",
    "    # save for future\n",
    "    month_by_ttwa_by_clusters[k_esco] = month_by_ttwa_by_cluster\n",
    "    month_by_ttwa_by_clusters_df[k_esco] = month_by_ttwa_by_cluster_df\n",
    "\n",
    "timer.end_task()\n",
    "SAVE_CLUSTER_TTWA_STOCK = False\n",
    "if SAVE_CLUSTER_TTWA_STOCK:\n",
    "    # save it\n",
    "    with gzip.GzipFile(f\"{DATA_PATH}/data/aux/stock_by_month_by_ttwa_by_cluster_all_{DATE_ID}.gz\", 'wb') as f:\n",
    "        pickle.dump((month_by_ttwa_by_clusters,month_by_ttwa_by_clusters_df),f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regional variation within skill clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_quotients_and_gini(data_to_use, col_to_use, biggest_employment_ttwas, data_df, \n",
    "                                 is_value_counts= False, indices_dict = None):\n",
    "    \"\"\" Compute location quotients for TTWAs, then compute the Gini coefficient of those location quotients\n",
    "    for each value of the variable of interest\"\"\"\n",
    "    # average across time\n",
    "    data_prep = data_to_use.mean()\n",
    "    # split the two dimensions across indices and columns\n",
    "    data_prep = data_prep.reset_index(level=1).rename(columns = {0:'stock'})\n",
    "    data_prep = data_prep.reset_index(drop=False)\n",
    "    \n",
    "    if not 'ttwa11cd' in data_prep.columns:\n",
    "        data_prep = data_prep.rename(columns = {'level_1': 'ttwa11cd'})\n",
    "    tmp = []\n",
    "    i=0\n",
    "    # var is the relevant dimension (SOC, skill cluster, industry, etc.)\n",
    "    for var in data_prep[col_to_use].value_counts().index:\n",
    "        tmp.append(data_prep[data_prep[col_to_use]==var].stock.values)\n",
    "        if i==0:\n",
    "            all_ttwas = data_prep[data_prep[col_to_use]==var].ttwa11cd\n",
    "        i+=1\n",
    "    #all_ttwas = data_prep[data_prep[col_to_use]==11.0].ttwa11cd\n",
    "    var_by_ttwa = pd.DataFrame(tmp, columns = all_ttwas, \n",
    "                 index= data_prep[col_to_use].value_counts().index)\n",
    "\n",
    "    # compute proportion of skill demand by \"variable\" across all UK\n",
    "    full_dist_in_uk = var_by_ttwa.sum(axis=1)/var_by_ttwa.sum().sum()\n",
    "    quotient_var_ttwa = pd.DataFrame(columns = var_by_ttwa.columns,\n",
    "                                    index = var_by_ttwa.index)\n",
    "\n",
    "    # get the location quotients for all TTWAs\n",
    "    for chosen_ttwa in var_by_ttwa.columns:\n",
    "        quotient_var_ttwa[chosen_ttwa] = (var_by_ttwa[chosen_ttwa]/var_by_ttwa[chosen_ttwa].sum()\n",
    "                                         )/full_dist_in_uk\n",
    "\n",
    "    # only keep largest TTWAs (population more than 60k)\n",
    "    quotient_var_ttwa = quotient_var_ttwa[biggest_employment_ttwas]\n",
    "\n",
    "    # now compute the Gini coefficient for each \"variable\"\n",
    "    gini_by_var = []\n",
    "    for chosen_var in var_by_ttwa.index:\n",
    "        gini_by_var.append(gini(quotient_var_ttwa.loc[chosen_var].to_numpy()))\n",
    "\n",
    "    quotient_var_ttwa= quotient_var_ttwa.assign(gini_index = gini_by_var)\n",
    "    \n",
    "    if is_value_counts:\n",
    "        quotient_var_ttwa = quotient_var_ttwa.assign(counts = data_df)\n",
    "    else:\n",
    "        quotient_var_ttwa = quotient_var_ttwa.assign(counts = data_df[col_to_use].value_counts())\n",
    "\n",
    "    if indices_dict:\n",
    "        quotient_var_ttwa.index = quotient_var_ttwa.index.map(lambda x: indices_dict[x])\n",
    "\n",
    "    print('Done')\n",
    "    \n",
    "    return var_by_ttwa, quotient_var_ttwa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, get the local quotients for each skillset in each TTWA - and then get the Gini index of the collection\n",
    "# of local quotients across TTWA for each skillset (at variuos levels of granularity)\n",
    "larger_rows_2_id = []\n",
    "for name in larger_rows_2.index:\n",
    "    if larger_rows_2.loc[name]:\n",
    "        ix = cluster_labels_2[cluster_labels_2.label==name.lower()].index[0]\n",
    "        larger_rows_2_id.append(ix)\n",
    "larger_rows_2_id = sorted(larger_rows_2_id)\n",
    "\n",
    "larger_rows_3_id = []\n",
    "for name in larger_rows_3.index:\n",
    "    if larger_rows_3.loc[name]:\n",
    "        ix = cluster_labels_3[cluster_labels_3.label==name.lower()].index[0]\n",
    "        larger_rows_3_id.append(ix)\n",
    "larger_rows_3_id = sorted(larger_rows_3_id)\n",
    "\n",
    "# cluster level 3\n",
    "skill3_by_ttwa, quotient_skill3_ttwa = get_local_quotients_and_gini(\n",
    "    pd.DataFrame.from_dict(month_by_ttwa_by_clusters_df['level_3']).loc[larger_rows_3_id].T, \n",
    "    'index', biggest_employment_ttwas40, stock_composition_3.mean(axis = 1), \n",
    "                is_value_counts = True, indices_dict = None)\n",
    "\n",
    "# cluster level 2\n",
    "skill2_by_ttwa, quotient_skill2_ttwa = get_local_quotients_and_gini(\n",
    "    pd.DataFrame.from_dict(month_by_ttwa_by_clusters_df['level_2']).loc[larger_rows_2_id].T, \n",
    "    'index', biggest_employment_ttwas40, stock_composition_2.mean(axis = 1), \n",
    "                is_value_counts = True, indices_dict = None)\n",
    "\n",
    "# cluster level 1\n",
    "skill1_by_ttwa, quotient_skill1_ttwa = get_local_quotients_and_gini(\n",
    "    pd.DataFrame.from_dict(month_by_ttwa_by_clusters_df['level_1']).T, \n",
    "    'index', biggest_employment_ttwas40, stock_composition_1.mean(axis = 1), \n",
    "                is_value_counts = True, indices_dict = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove small clusters\n",
    "largest_clusters_2 = skill2_by_ttwa.T.sum()>0\n",
    "print('Number of clusters level 2 included: ',largest_clusters_2.sum())\n",
    "\n",
    "largest_clusters_3 = skill3_by_ttwa.T.sum()>0\n",
    "print('Number of clusters level 3 included: ',largest_clusters_3.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of Gini indices\n",
    "with sns.plotting_context('talk'):\n",
    "    # clusters level 3\n",
    "    plt.figure()\n",
    "    sns.distplot(quotient_skill3_ttwa.loc[largest_clusters_3,'gini_index'].dropna(), color = nesta_colours[2])\n",
    "    print('Clusters level 3')\n",
    "    plt.ylabel('Skill clusters counts')\n",
    "    plt.xlabel('Gini index')\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(f\"{output_folder}/gini_indices_for_skill_clusters_level3_over_ttwa.svg\")\n",
    "        plt.savefig(f\"{output_folder}/gini_indices_for_skill_clusters_level3_over_ttwa.png\")\n",
    "    \n",
    "    # clusters level 2\n",
    "    plt.figure()\n",
    "    sns.distplot(quotient_skill2_ttwa.loc[largest_clusters_2,'gini_index'].dropna(), color = nesta_colours[2])\n",
    "    print('Clusters level 2')\n",
    "    plt.ylabel('Skill clusters counts')\n",
    "    plt.xlabel('Gini index')\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(f\"{output_folder}/gini_indices_for_skill_clusters_level2_over_ttwa.svg\")\n",
    "        plt.savefig(f\"{output_folder}/gini_indices_for_skill_clusters_level2_over_ttwa.png\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest Gin indices at level 3: these are the skill clusters with higher variation across the UK\n",
    "print('Clusters level 3. Highest Gini indices')\n",
    "clusters_high_gini = quotient_skill3_ttwa.loc[largest_clusters_3,['gini_index','counts']].sort_values('gini_index', \n",
    "    ascending=False).head(10).rename(index = lambda x: cluster_labels_3.loc[x].label)\n",
    "\n",
    "print(clusters_high_gini.index.tolist())\n",
    "printdf(clusters_high_gini)\n",
    "\n",
    "\n",
    "# lowest Gini indices at level 3: these are the skill clusters with lower variation across the UK\n",
    "print('Lowest Gini indices')\n",
    "clusters_low_gini =quotient_skill3_ttwa.loc[largest_clusters_3, ['gini_index','counts']].sort_values('gini_index', \n",
    "    ascending=True).head(10).rename(index = lambda x: cluster_labels_3.loc[x].label)\n",
    "print(clusters_low_gini.index.tolist())\n",
    "\n",
    "printdf(clusters_low_gini)\n",
    "\n",
    "# print ratio of max gini over min gini\n",
    "print(quotient_skill3_ttwa.loc[largest_clusters_3].gini_index.max(),\n",
    "     quotient_skill3_ttwa.loc[largest_clusters_3].gini_index.min())\n",
    "max_over_min = quotient_skill3_ttwa.loc[largest_clusters_3].gini_index.max(\n",
    "    )/quotient_skill3_ttwa.loc[largest_clusters_3].gini_index.min()\n",
    "print(f'Max/min is {max_over_min}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest Gini indices at level 2: these are the skill clusters with higher variation across the UK\n",
    "quotient_skill2_ttwa[['gini_index','counts']\n",
    "                    ][quotient_skill2_ttwa.counts>0].sort_values('gini_index', ascending=False).rename(\n",
    "    index = lambda x: cluster_labels_2.loc[x].label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Gini indices at level 1, in decreasing order\n",
    "quotient_skill1_ttwa[['gini_index','counts']\n",
    "                    ][quotient_skill1_ttwa['counts']>10000].sort_values('gini_index', ascending=False).rename(\n",
    "    index = lambda x: cluster_labels_1.loc[x].label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do location quotients for skill clusters correlate with those for industries?\n",
    "industry_by_cluster = pd.merge(quotient_sic_ttwa.T, quotient_skill2_ttwa.T, \n",
    "                               left_index=True, right_index= True, how='inner')\n",
    "\n",
    "variables_to_keep = industry_by_cluster.loc['counts']>5000\n",
    "\n",
    "industry_by_cluster = industry_by_cluster[variables_to_keep[variables_to_keep].index]\n",
    "\n",
    "industry_by_cluster = industry_by_cluster.drop(['gini_index','counts']).rename(columns = \n",
    "                    {x: cluster_labels_2.loc[x].label for x in cluster_labels_2.index}).corr()\n",
    "\n",
    "industry_by_cluster_red = industry_by_cluster.iloc[:21,21:]\n",
    "\n",
    "with sns.plotting_context('talk'):\n",
    "    f = plt.figure(figsize = (18,11))\n",
    "    sns.heatmap(industry_by_cluster_red.applymap(lambda x: np.around(x,2)), ax = f.gca(), \n",
    "                cmap = sns.color_palette(\"RdYlBu\", 10), vmin=-1, vmax = 1)#, annot = True, fmt = '.2f')\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(f\"{output_folder}/industry_cluster_level_2_locquotient_correlation.png\")\n",
    "        plt.savefig(f\"{output_folder}/industry_cluster_level_2_locquotient_correlation.svg\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_by_cluster = pd.merge(quotient_sic_ttwa.T, quotient_skill1_ttwa.T, \n",
    "                               left_index=True, right_index= True, how='inner')\n",
    "\n",
    "variables_to_keep = industry_by_cluster.loc['counts']>5000\n",
    "industry_by_cluster = industry_by_cluster[variables_to_keep[variables_to_keep].index]\n",
    "\n",
    "industry_by_cluster = industry_by_cluster.drop(['gini_index','counts']).rename(columns = \n",
    "                    {x: cluster_labels_1.loc[x].label for x in cluster_labels_1.index}).corr()\n",
    "\n",
    "industry_by_cluster_red = industry_by_cluster.iloc[:15,15:]\n",
    "\n",
    "with sns.plotting_context('talk'):\n",
    "    f = plt.figure(figsize = (14,10))\n",
    "    sns.heatmap(industry_by_cluster_red.applymap(lambda x: np.around(x,2)), ax = f.gca(), \n",
    "                cmap = sns.color_palette(\"RdYlBu\", 10), vmin=-1, vmax = 1)#, annot = True, fmt = '.2f')\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(f\"{output_folder}/industry_cluster_level_1_locquotient_correlation.png\")\n",
    "        plt.savefig(f\"{output_folder}/industry_cluster_level_1_locquotient_correlation.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top TTWAs for some skills clusters level 2\n",
    "for cluster_id in quotient_skill2_ttwa.sort_values(by='counts', ascending=False).head(40).index:\n",
    "    print(f\"TTWA with highest local quotient for second level cluster '{cluster_labels_2.loc[cluster_id].label}'\")\n",
    "    tmp = quotient_skill2_ttwa.loc[cluster_id].sort_values(ascending=False).drop(['gini_index','counts']).rename(\n",
    "        lambda x: ttwa_code2name[x])\n",
    "    print(tmp.head(5).index.tolist())\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top TTWAs for some clusters level 1\n",
    "for cluster_id in quotient_skill1_ttwa.sort_values(by='counts', ascending=False).head(10).index:\n",
    "    print(f\"TTWA with highest local quotient for second level cluster '{cluster_labels_1.loc[cluster_id].label}'\")\n",
    "    tmp = quotient_skill1_ttwa.loc[cluster_id].sort_values(ascending=False).drop(['gini_index','counts']).rename(\n",
    "        lambda x: ttwa_code2name[x])\n",
    "    print(tmp.head(5).index.tolist())\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variable = quotient_skill2_ttwa.rename(lambda x: cluster_labels_2.loc[x].label)\n",
    "for select_ttwa in ['London','Aberdeen','Belfast','Edinburgh']:\n",
    "    print(select_ttwa)\n",
    "    print(test_variable[ttwa_name2code[select_ttwa]].sort_values(ascending= True).head(10))\n",
    "    print(test_variable[ttwa_name2code[select_ttwa]].sort_values(ascending= False).head(10))\n",
    "    print('-'*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of skills within skill clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skill cluster to check: \n",
    "[programming, Assembly and maintenance, import & export management, digital marketing, logistics, 'food preparation', 'aquaculture work', 'user interface', 'metal processing', 'curriculum management', 'policy', 'public relations & campaigning', 'import & export management', 'human resources', 'forensics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters_for_demand = ['programming', 'assembly & maintenance', 'import & export management', \n",
    "                       'digital marketing', 'marketing', 'logistics', 'general core skills','subject teaching']\n",
    "clusters_for_demand += ['warehousing', 'subject teaching', 'machine operation', 'programming', 'food preparation', \n",
    "                      'web development', 'nursing', 'warehouse management & distribution', 'law', 'ict security',\n",
    "                      'aquaculture work']\n",
    "clusters_for_demand += ['shop management', 'logistics', 'merchandising', 'sales', 'secretarial assistance', \n",
    "                     'call centre services', 'business management', 'insurance', 'surveying',\n",
    "                     'import & export management', \n",
    "                    'human resources', 'forensics']\n",
    "for skill_clus in clusters_for_demand:\n",
    "    # print most common 15 skills\n",
    "    top_skills = tk_esco_121[tk_esco_121.cluster_label_level_3==skill_clus].sort_values(\n",
    "        by='tk_counts', ascending=False).head(15).index.to_list()\n",
    "    print(skill_clus)\n",
    "    print(top_skills)\n",
    "    print('-'*70)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the supplementary material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition by industry, location, occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print composition by industry, location, occupation\n",
    "# Note that job adverts for which data is missing are not included when computing shares.\n",
    "# Mining is also removed because there are too few job adverts matched to this industry\n",
    "\n",
    "tables_folder = '/path/to/tables'\n",
    "for k in ['sic_only', 'ttwa_only', 'soc1_only', 'soc2_only', 'soc3_only', 'soc4_only']:\n",
    "    active_stock = stocks_per_month[k].resample('Y').mean().T\n",
    "    if k=='ttwa_only':\n",
    "        active_stock = active_stock.rename(columns = {pd.to_datetime(f\"{t}-12-31\"): f\"{t}\" \n",
    "                                    for t in ['2015','2016','2017','2018','2019']})\n",
    "        # Divide by 100 economically active residents\n",
    "        active_stock= active_stock.drop(['not_found'])\n",
    "        active_stock = active_stock.merge(employment_levels, left_on='ttwa11cd', right_index=True)\n",
    "        for year in ['2015','2016','2017','2018','2019']:\n",
    "            year_index = pd.to_datetime(f\"31-12-{year}\")\n",
    "            active_stock[year] = active_stock[year]/active_stock[f\"employment_{year}\"]*100\n",
    "            active_stock = active_stock.rename(columns = {year: f'{year} (normalised stock)'})\n",
    "        active_stock = active_stock[[f\"{year} (normalised stock)\" for year in \n",
    "                                     ['2015','2016','2017','2018','2019']]]\n",
    "        # only select the biggest TTWAs\n",
    "        active_stock = active_stock[active_stock.index.map(lambda x:\n",
    "                            x in biggest_employment_ttwas40)] \n",
    "        \n",
    "        active_stock = active_stock.rename(ttwa_code2name)\n",
    "        savename = 'annual_normalised_stock_of_demand_by_ttwa'\n",
    "        df_name = 'TTWA name'\n",
    "        active_stock = active_stock.rename_axis(df_name, axis = 'index')\n",
    "    else:\n",
    "        active_stock = active_stock.rename(columns = {pd.to_datetime(f\"{t}-12-31\"): f\"{t} (%)\" \n",
    "                                    for t in ['2015','2016','2017','2018','2019']})\n",
    "        if k == 'sic_only':\n",
    "            active_stock= active_stock.drop(['uncertain','B'])\n",
    "            active_stock = active_stock.rename(sic_letter_to_text)\n",
    "            sheet_name = 'vacancies by industry'\n",
    "            savename = 'annual_composition_of_demand_by_industry'\n",
    "            df_name = 'SIC 2007'\n",
    "        elif 'soc' in k:\n",
    "            group_type = {'soc1_only': 'major_groups', 'soc2_only': 'sub-major_groups', \n",
    "                          'soc3_only': 'minor_groups', 'soc4_only': 'unit_groups'}[k]\n",
    "            savename = f'annual_composition_of_demand_by_occupation_{group_type}'\n",
    "            sheet_name = f\"vacancies by {group_type.replace('_',' ')}\"\n",
    "            df_name = 'Occupation'\n",
    "        active_stock =active_stock/active_stock.sum()*100\n",
    "        active_stock = active_stock.rename_axis(df_name, axis = 'index')\n",
    "    \n",
    "    # round to 2 digits\n",
    "    active_stock = active_stock.applymap(lambda x: np.around(x,2))\n",
    "    # save\n",
    "    #active_stock.to_csv(f\"{tables_folder}/{savename}.csv\")\n",
    "    \n",
    "    if k=='ttwa_only':\n",
    "        active_stock.to_csv(f\"{tables_folder}/{savename}.csv\")\n",
    "    elif 'soc' in k:\n",
    "        tmp = group_type.replace('_',' ')\n",
    "        active_stock[f\"SOC 2010 ({tmp})\"] = active_stock.index.map(lambda x: int(x))\n",
    "        active_stock = active_stock.rename(socnames_dict).reset_index()\n",
    "        active_stock = active_stock[[f\"SOC 2010 ({tmp})\",'Occupation','2015 (%)','2016 (%)',\n",
    "                                    '2017 (%)','2018 (%)','2019 (%)']]\n",
    "        active_stock.to_csv(f\"{tables_folder}/{savename}.csv\", index= False)\n",
    "    else:\n",
    "        if k == 'sic_only':\n",
    "            continue\n",
    "        active_stock.to_csv(f\"{tables_folder}/{savename}.csv\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition by skill clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in month_by_clusters.keys():\n",
    "    \n",
    "    active_stock = pd.DataFrame(month_by_clusters[k].resample('Y').mean().T)\n",
    "    active_stock = active_stock.rename(columns = {pd.to_datetime(f\"{t}-12-31\"): f\"{t} (%)\" \n",
    "                                for t in ['2015','2016','2017','2018','2019']})\n",
    "    cluster_to_drop = {'level_1':15, 'level_2': 76, 'level_3': 201}[k]\n",
    "    active_stock = active_stock.drop(cluster_to_drop)\n",
    "    cluster_labels_active = {'level_1': cluster_labels_1, 'level_2': cluster_labels_2,\n",
    "                            'level_3': cluster_labels_3}[k]\n",
    "    active_level = k[-1]\n",
    "    active_stock = active_stock.rename(lambda x: cluster_labels_active.loc[int(x)].label.capitalize())\n",
    "    sheet_name = 'vacancies by skill category'\n",
    "    savename = f'annual_composition_of_demand_by_skill_category_level_{active_level}'\n",
    "    df_name = f'Skill category (level {active_level})'\n",
    "    active_stock =active_stock/active_stock.sum()*100\n",
    "    active_stock = active_stock.rename_axis(df_name, axis = 'index')\n",
    "    \n",
    "    active_stock = active_stock.sort_values(by = '2015 (%)', ascending = False)\n",
    "    #save column names\n",
    "    \n",
    "    # round to 2 digits\n",
    "    active_stock = active_stock.applymap(lambda x: np.around(x,2))\n",
    "    # save\n",
    "    active_stock.to_csv(f\"{tables_folder}/{savename}.csv\")\n",
    "    \n",
    "    #active_stock.to_csv(f\"{tables_folder}/{savename}.csv\")\n",
    "    printdf(active_stock.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Internal use only] Get SIC by SOC breakdown from job adverts matched via TK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_add = pd.read_csv(f\"{DATA_PATH}/data/interim/interim_job_id_and_vacancy_weights.gz\", \n",
    "    compression = 'gzip', encoding = 'utf-8', usecols = ['posting_id','organization_industry_value'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_map_industry_label_values():\n",
    "    \"\"\" Get a map defining which TK ID value for sector corresponds to which label\"\"\"\n",
    "    tmp_data = pd.read_csv(os.path.join(f\"{DATA_PATH}/data/processed\", \n",
    "                                        tk_params.file_name_template.format(0)), \n",
    "                           compression='gzip',\n",
    "                encoding = 'utf-8',usecols = ['organization_industry_label',\n",
    "                                              'organization_industry_value'])\n",
    "    map_label2value = {}\n",
    "    map_value2label = {}\n",
    "    for name,g in tmp_data.groupby('organization_industry_value'):\n",
    "        map_value2label[name] = g.organization_industry_label.value_counts().index.values[0]\n",
    "        map_label2value[map_value2label[name]] = name\n",
    "    return map_label2value, map_value2label\n",
    "\n",
    "# create the maps\n",
    "map_label2value, map_value2label = get_map_industry_label_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which TK categories are best matched to SIC codes (g = good, a =ambiguous)\n",
    "assessment_map_tk2sic = {5: 'g', \n",
    "                      19: 'g',\n",
    "                      11 : 'g',\n",
    "                      8: 'g',\n",
    "                      13: 'g',\n",
    "                      6: 'g',\n",
    "                      9: 'g',\n",
    "                      10: 'g', \n",
    "                    12: 'g',\n",
    "                    1: 'g',\n",
    "                    2:'g',\n",
    "                     16:'a',\n",
    "                    7:'g',\n",
    "                    14:'g',\n",
    "                    21:'a',\n",
    "                    18:'g',\n",
    "                    17:'a',\n",
    "                    4:'a',\n",
    "                    15:'a',\n",
    "                    20:'u',\n",
    "                    0: 'u',\n",
    "                    3: 's'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_add['industry_type_label'] = data_to_add.organization_industry_value.map(\n",
    "    lambda x: assessment_map_tk2sic[x])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_add = data_to_add.merge(data_df[['posting_id','profession_soc_code_value',\n",
    "                                         'final_sic_letter']], \n",
    "                                on ='posting_id', how='left')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic_soc_ga_breakdown = data_to_add[data_to_add.industry_type_label.map(lambda x: x in ['g','a'])]\n",
    "sic_soc_ga_breakdown = pd.crosstab(sic_soc_ga_breakdown['profession_soc_code_value'],\n",
    "                                  sic_soc_ga_breakdown['final_sic_letter'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic_soc_ga_breakdown.to_csv(\n",
    "    '/Users/stefgarasto/Local-Data/textkernel/results/sic_matches/sic_soc_breakdown_from_select_adverts.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise to 1000 top\n",
    "sic_soc_ga_breakdown = 1000*sic_soc_ga_breakdown/sic_soc_ga_breakdown.max()\n",
    "# save again\n",
    "sic_soc_ga_breakdown.to_csv(\n",
    "    '/Users/stefgarasto/Local-Data/textkernel/results/sic_matches/sic_soc_breakdown_from_select_adverts_to_share.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
