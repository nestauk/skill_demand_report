{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of duration and SOC x skill cluster crosswalk\n",
    "\n",
    "The goal is to a) get some basics statistics on the distribution of the duration of job adverts (it creates one of the figures in the report); b) assess whether there are any differences in duration across occupations, industries, skill categories and job titles, c) show some examples of skill categories needed for selected occupations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ DEPENDENCIES AND FUNCTIONS ------------------------\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from pprint import PrettyPrinter\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from scipy.special import gammaln\n",
    "import json\n",
    "\n",
    "#sys.path.append('/Users/stefgarasto/Google Drive/Documents/scripts/utils_stef')\n",
    "#from utils_skills_clusters import taxonomy_2_0\n",
    "sys.path.append(\"/Users/stefgarasto/Local-Data/scripts/skill_demand_escoe/skill_demand/skill_demand\")\n",
    "from utils.utils_general import TaskTimer, print_elapsed, nesta_colours, sic_letter_to_text, flatten_lol, printdf, socnames_dict\n",
    "from utils.textkernel_load_utils import tk_params, DATA_PATH, create_tk_import_dict, read_and_append_chunks\n",
    "\n",
    "'''\n",
    "#from textkernel_utils import *\n",
    "sys.path.append(\"/Users/stefgarasto/Local-Data/scripts/skill_demand_escoe/skill_demand/skill_demand\")\n",
    "\n",
    "from utils.textkernel_load_utils import tk_params, data_folder, \\\n",
    "        create_tk_import_dict, read_and_append_chunks\n",
    "from time import time as tt\n",
    "#from tqdm import tqdm\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "from utils_skills_clusters import taxonomy_2_0\n",
    "now = datetime.datetime.now()\n",
    "sys.path.append('/Users/stefgarasto/Google Drive/Documents/scripts/utils_stef')\n",
    "from utils.utils_general import TaskTimer, print_elapsed, nesta_colours, \\\n",
    "            sic_letter_to_text, flatten_lol, printdf, socnames_dict\n",
    "'''\n",
    "pp = PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "timer = TaskTimer()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from flow_to_stock.flow_to_stock_funcs import load_ons_vacancies\n",
    "#from flow_to_stock_funcs import load_ons_vacancies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIC mapping between text and letters\n",
    "\n",
    "sic_letter_to_text['Z'] = 'Others'\n",
    "sic_letter_to_text['L_O_S'] = 'Personal and public services'#including non-profit and estate agents\n",
    "sic_letter_to_text['D_E'] = 'Utilities (energy, water and waste)'\n",
    "sic_letter_to_text['M_P'] = 'Educational and professional activities'\n",
    "sic_letter_to_text['uncertain'] = 'Uncertain'\n",
    "\n",
    "sic_text_to_letter= {}\n",
    "for letter in sic_letter_to_text.keys():\n",
    "    sic_text= sic_letter_to_text[letter]\n",
    "    sic_text_to_letter[sic_text] = letter\n",
    "    \n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = Path(data_folder).parent\n",
    "#assert(DATA_PATH == Path('/Volumes/ssd_data/textkernel/data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_soc_to_n_digits(soc_code,n=3):\n",
    "    if np.isnan(soc_code):\n",
    "        return np.nan\n",
    "    else:\n",
    "        m = {1: 1000, 2: 100, 3: 10}[n]\n",
    "        return (soc_code - soc_code%m)/m\n",
    "\n",
    "def resample_soc_to_n_digits_df(soc_code_df,n=3):\n",
    "    #soc_code_df = soc_code_df.fillna(0)\n",
    "    m = {1: 1000, 2: 100, 3: 10}[n]\n",
    "    return (soc_code_df - soc_code_df%m)/m\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------------------------------------------------------------------------\n",
    "#              Main functions to convert from flow to stock\n",
    "#% --------------------------------------------------------------------------\n",
    "\n",
    "def set_month_at_beginning(x):\n",
    "    \"\"\"Set a datetime to the beginning of the month\"\"\"\n",
    "    return pd.offsets.MonthBegin(0).rollback(x)\n",
    "\n",
    "def set_month_at_end(x):\n",
    "    \"\"\" Set a datetime to the end of the month\"\"\"\n",
    "    return pd.offsets.MonthEnd(0).rollforward(x)\n",
    "\n",
    "\n",
    "def get_daily_stock_breakdown(data, agg_func = 'sum', agg_col = 'vacancy_weight', \n",
    "                              breakdown_col = 'organization_industry_value', BOUNDARY = None):\n",
    "    \"\"\"Compute the daily stock of vacancies via cumulative sum of net Flow.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- dataframe with online job vacancies. Need to have \"date\", \n",
    "            \"end_date\" and agg_col columns\n",
    "    agg_func: whether to count the vacancies or to sum the weights\n",
    "    agg_col -- reference column to aggregate (usually column with per-vacancy weights)\n",
    "    BOUNDARY -- what to do wrt boundary conditions (start and end month)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(breakdown_col,list):\n",
    "        breakdown_col = [breakdown_col]\n",
    "        \n",
    "    start_day = data.date.min()\n",
    "    end_day = data.date.max()\n",
    "    \n",
    "    print(agg_func)\n",
    "    if agg_func == 'sum':\n",
    "        vacancy_flow_per_day = data.groupby(['date'] + breakdown_col)[agg_col].sum()\n",
    "        vacancy_remove_per_day = data.groupby(['end_date']+ breakdown_col)[agg_col].sum()\n",
    "    else:\n",
    "        vacancy_flow_per_day = data.groupby(['date'] + breakdown_col)[agg_col].count()\n",
    "        vacancy_remove_per_day = data.groupby(['end_date'] + breakdown_col)[agg_col].count()\n",
    "    \n",
    "    for _ in range(len(breakdown_col)):\n",
    "        vacancy_flow_per_day = vacancy_flow_per_day.unstack()\n",
    "        vacancy_remove_per_day = vacancy_remove_per_day.unstack()\n",
    "    \n",
    "    # shift vacancy_remove_per_day by one day since vacancies disappear the day\n",
    "    # after their expiration date\n",
    "    vacancy_remove_per_day = vacancy_remove_per_day.shift(1)\n",
    "    \n",
    "    # adjust so that they start and end on the same dates\n",
    "    vacancy_flow_per_day = vacancy_flow_per_day.reindex(pd.date_range(start=start_day,\n",
    "                                        end=end_day,freq='D'), fill_value=0)#, level=0)\n",
    "    vacancy_remove_per_day = vacancy_remove_per_day.reindex(pd.date_range(start=start_day,\n",
    "                                        end=end_day,freq='D'), fill_value=0)#, level=0)\n",
    "    \n",
    "    # compute the net Flow\n",
    "    net_flow = vacancy_flow_per_day.fillna(0) - vacancy_remove_per_day.fillna(0)\n",
    "    \n",
    "    # Get the daily stock\n",
    "    daily_stock = net_flow.cumsum()\n",
    "    \n",
    "    # Resample to monthly stock\n",
    "    monthly_stock = net_flow.resample('M').sum().cumsum()/2\n",
    "    monthly_stock.index = monthly_stock.index.map(set_month_at_beginning)\n",
    "    \n",
    "    # enforce boundary conditions\n",
    "    if BOUNDARY == 'valid':\n",
    "        monthly_stock = monthly_stock[monthly_stock.index>=set_month_at_beginning(\n",
    "            pd.to_datetime(FIRST_VALID_MONTH))]\n",
    "    return monthly_stock, daily_stock, vacancy_flow_per_day, vacancy_remove_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # based on bottom eq:\n",
    "    # http://www.statsdirect.com/help/generatedimages/equations/equation154.svg\n",
    "    # from:\n",
    "    # http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    # All values are treated equally, arrays must be 1d:\n",
    "    array = array.flatten()\n",
    "    if np.amin(array) < 0:\n",
    "        # Values cannot be negative:\n",
    "        array -= np.amin(array)\n",
    "    # Values cannot be 0:\n",
    "    array += 0.0000001\n",
    "    # Values must be sorted:\n",
    "    array = np.sort(array)\n",
    "    # Index per array element:\n",
    "    index = np.arange(1,array.shape[0]+1)\n",
    "    # Number of array elements:\n",
    "    n = array.shape[0]\n",
    "    # Gini coefficient:\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load import dict and other auxiliary variables\n",
    "print(tk_params)\n",
    "# define names of files to load\n",
    "N_to_load = tk_params.N_files\n",
    "indices_to_load = np.random.permutation(tk_params.N_files)[:N_to_load]\n",
    "dfilenames = [os.path.join(f\"{DATA_PATH}data\",tk_params.file_name_template.format(i)) for i in\n",
    "              indices_to_load]\n",
    "import_dict, dates_to_parse = create_tk_import_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_dict['job_title'], dates_to_parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the dictionary\n",
    "import_dict['final_sic_letter'] = 'category'\n",
    "import_dict['clean_organization_name'] = 'string'\n",
    "import_dict['duration_to_use'] = 'float64'\n",
    "import_dict['vacancy_weight'] = 'float64'\n",
    "import_dict['active_months'] = 'string'\n",
    "import_dict['best_month'] = 'string'\n",
    "import_dict['best_month_duration'] = 'float64'\n",
    "import_dict['ttwa11cd'] = 'category'\n",
    "import_dict['top_cluster'] = 'float64'\n",
    "import_dict['top_cluster_weighted'] = 'float64'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data on vacancy lifecycle, per vacancy weights, sector and profession\n",
    "\n",
    "job_id, posting_id, date, duration, profession, tk_industry, organization name (original and clean), sic, duration used, vacancy weight, active month, best month, best month duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.start_task('Loading SIC + vacancy weights')\n",
    "data_df = pd.read_csv(f\"{DATA_PATH}data/interim/interim_job_id_and_vacancy_weights.gz\", \n",
    "    compression = 'gzip', encoding = 'utf-8', \n",
    "                      usecols = ['posting_id','date','duration_to_use','duration',\n",
    "                                'clean_organization_name','profession_soc_code_value',\n",
    "                                'final_sic_letter'],#,'vacancy_weight','best_month_duration'],\n",
    "                      dtype = import_dict, \n",
    "                      parse_dates = ['date'])\n",
    "timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_length_df = len(data_df)\n",
    "print(original_length_df)\n",
    "data_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['end_date'] = data_df.date + pd.to_timedelta(\n",
    "        data_df.duration_to_use - 1, unit='D')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and join location data\n",
    "\n",
    "job id, posting id, ttwa and region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timer.start_task('Load location data')\n",
    "data_to_add = pd.read_csv(f\"{DATA_PATH}data/interim/job_id_and_ttwa11cd_first_batch.gz\",\n",
    "                            #interim_location_data.gz\", \n",
    "                        compression = 'gzip', encoding = 'utf-8', dtype = import_dict, \n",
    "                        usecols = ['posting_id','region_label','ttwa11cd'])\n",
    "timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_add.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.start_task('Joining with location dataframe')\n",
    "data_df = data_df.merge(data_to_add, \n",
    "                            on ='posting_id', how='left')\n",
    "timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(data_df) == original_length_df)\n",
    "print(data_df.ttwa11cd.notna().sum(), len(data_to_add))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load helpers for geospatial mapping\n",
    "\n",
    "1. TTWA dictionary to go from code to name\n",
    "2. TTWA statistics\n",
    "3. TTWA shapefiles\n",
    "4. Regions shapefiles\n",
    "5. Country shapefiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttwa_folder = ('/Users/stefgarasto/Local-Data/scripts/skill_demand_escoe/skill_demand/'\n",
    "               'data/aux/ONS/Travel_to_Work_Areas_2011_guidance_and_information_V4')\n",
    "ttwa_file_1 = 'TTWA-info-2016.xls'\n",
    "ttwa_file_2 = 'TTWA-summary-statistics-2011.xls'\n",
    "ttwa_stats = pd.read_excel(f\"{ttwa_folder}/{ttwa_file_1}\")\n",
    "tmp = pd.read_excel(f\"{ttwa_folder}/{ttwa_file_2}\")\n",
    "ttwa_stats = ttwa_stats.merge(tmp, on =['ttwa11cd','ttwa11nm'], how= 'left')\n",
    "ttwa_code2name = dict(zip(ttwa_stats.ttwa11cd,ttwa_stats.ttwa11nm))\n",
    "ttwa_name2code = dict(zip(ttwa_stats.ttwa11nm,ttwa_stats.ttwa11cd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttwa_name2code['London']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttwa_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_code = 'E30000234'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_ttwas = ttwa_stats.ttwa11cd[ttwa_stats[' Population']>60000].to_list()\n",
    "london_code in biggest_ttwas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load information on top cluster for each job advert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pieces with posting id, soc codes and top clusters\n",
    "LOAD_TOP_CLUSTERS= True\n",
    "if LOAD_TOP_CLUSTERS:\n",
    "    all_cluster_files = os.listdir(f\"{DATA_PATH}data/interim/soc_by_clusters_no_soft_skills\")\n",
    "    all_cluster_files = sorted([t for t in all_cluster_files if 'best_clusters_with_soc' in t])\n",
    "    print(len(all_cluster_files), all_cluster_files[:4])\n",
    "\n",
    "    timer.start_task('Loading top clusters')\n",
    "    data_to_add = []\n",
    "    for ix,cluster_file in enumerate(all_cluster_files):\n",
    "        data_to_add.append(pd.read_csv(f\"{DATA_PATH}data/interim/soc_by_clusters_no_soft_skills/{cluster_file}\",\n",
    "                                       compression = 'gzip', encoding = 'utf-8', \n",
    "                                       usecols = ['posting_id','top_cluster','top_cluster_weighted'], \n",
    "                                       dtype= import_dict))\n",
    "\n",
    "    data_to_add = pd.concat(data_to_add)\n",
    "    timer.end_task()\n",
    "\n",
    "    print(data_to_add.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_TOP_CLUSTERS:\n",
    "    print(len(data_to_add))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_TOP_CLUSTERS:\n",
    "    # merge with main dataframe\n",
    "    timer.start_task('Joining with location dataframe')\n",
    "    data_df = data_df.merge(data_to_add, \n",
    "                                on ='posting_id', how='left')\n",
    "    timer.end_task()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and join clean job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_JOB_TITLES= True\n",
    "# ADD cols to load\n",
    "if LOAD_JOB_TITLES:\n",
    "    timer.start_task('Load cleaned job titles')\n",
    "    data_to_add = pd.read_csv(f\"{DATA_PATH}data/interim/interim_job_id_and_cleaned_titles.gz\", \n",
    "        compression = 'gzip', encoding = 'utf-8', dtype = import_dict, \n",
    "        usecols = ['posting_id','job_title_processed'])\n",
    "    timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_JOB_TITLES:\n",
    "    print(len(data_to_add),original_length_df)\n",
    "    assert(len(data_to_add) == original_length_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_JOB_TITLES:\n",
    "    timer.start_task('Joining cleaned job titles with main dataframe')\n",
    "    data_df = data_df.merge(\n",
    "        data_to_add, on ='posting_id', how = 'left')\n",
    "    timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_JOB_TITLES:\n",
    "    assert(len(data_df) == original_length_df)\n",
    "    print(data_df.job_title_processed.notna().sum(), \n",
    "          data_to_add.job_title_processed.notna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few more bits\n",
    "# release memory\n",
    "data_to_add = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dtype to make some computations faster\n",
    "data_df.profession_soc_code_value = data_df.profession_soc_code_value.astype('float')\n",
    "data_df.final_sic_letter = data_df.final_sic_letter.astype('string')\n",
    "data_df.ttwa11cd = data_df.ttwa11cd.astype('string')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SOC to skill cluster crosswalks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_ID = 'July2020'\n",
    "with gzip.GzipFile(f\"{DATA_PATH}/data/aux/final_all_crosswalks_soc_to_clusters_top_avg_{DATE_ID}.gz\",'rb') as f:\n",
    "    soc_by_clusters = pickle.load(f)\n",
    "\n",
    "# make sure everything is a float and not an int\n",
    "for k1 in soc_by_clusters.keys():\n",
    "    for k2 in soc_by_clusters[k1].keys():\n",
    "        soc_by_clusters[k1][k2] = soc_by_clusters[k1][k2]+0.0\n",
    "\n",
    "# normalise all crosswalk so that it sums up to 1 for each occupation\n",
    "timer.start_task('normalising crosswalks')\n",
    "soc_by_clusters_norm = {}\n",
    "for year in soc_by_clusters.keys():\n",
    "    soc_by_clusters_norm[year] = {}\n",
    "    for k in soc_by_clusters[year].keys():\n",
    "#            k = f\"{k_soc}d_by_{k_esco}\"\n",
    "        tmp = deepcopy(soc_by_clusters[year][k])\n",
    "        tmp = tmp/tmp.sum()\n",
    "        soc_by_clusters_norm[year][k] = tmp\n",
    "        \n",
    "timer.end_task()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load auxiliary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the esco list, including skills that have not been clustered\n",
    "res_folder_local = '/Users/stefgarasto/Local-Data/textkernel/results/skills_matches'\n",
    "\n",
    "# Load full clusters\n",
    "esco_clusters_dir = f'{DATA_PATH}data/aux'\n",
    "esco_clusters_file = os.path.join(esco_clusters_dir,\n",
    "             'ESCO_Essential_clusters_May2020_coreness.csv')\n",
    "esco_clusters = pd.read_csv(esco_clusters_file)\n",
    "# make alt labels list\n",
    "esco_clusters['alt_labels'] = esco_clusters.alt_labels.map(\n",
    "        lambda x: x.split('\\n') if isinstance(x,str) else [])\n",
    "\n",
    "# adjustments to the labels\n",
    "esco_clusters.loc[esco_clusters.level_2==20.0,'label_level_2'] = 'land transport (rail)'\n",
    "esco_clusters.loc[esco_clusters.level_3==191.0,'label_level_3'\n",
    "                 ] = 'leather production (manufacturing)'\n",
    "esco_clusters.loc[esco_clusters.level_3==190.0,'label_level_3'] = 'footwear design'\n",
    "esco_clusters.loc[esco_clusters.level_3==57.0,'label_level_3'] = 'marketing (branding)'\n",
    "\n",
    "print('All ESCO skills: ', len(esco_clusters))\n",
    "print('Valid ESCO skills: ',len(esco_clusters[esco_clusters.level_1<15]))\n",
    "\n",
    "print(esco_clusters.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cluster labels\n",
    "cluster_labels_1 = pd.read_csv(f\"{esco_clusters_dir}/ESCO_Essential_clusters_Level_1.csv\")\n",
    "cluster_labels_2 = pd.read_csv(f\"{esco_clusters_dir}/ESCO_Essential_clusters_Level_2.csv\")\n",
    "cluster_labels_3 = pd.read_csv(f\"{esco_clusters_dir}/ESCO_Essential_clusters_Level_3.csv\")\n",
    "\n",
    "cluster_labels_3 = cluster_labels_3.set_index('level_3')\n",
    "cluster_labels_2 = cluster_labels_2.set_index('level_2')\n",
    "\n",
    "# correct for same labels!!\n",
    "cluster_labels_3.loc[191,'label'] = 'leather production (manufacturing)'\n",
    "cluster_labels_3.loc[190,'label'] = 'footwear design'\n",
    "cluster_labels_3.loc[57,'label'] = 'marketing (branding)'\n",
    "cluster_labels_2.loc[20,'label'] = 'land transport (rail)'\n",
    "\n",
    "cluster_labels_3.tail(12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the crosswalks between levels\n",
    "esco_first_to_second={}\n",
    "esco_second_to_first={}\n",
    "for name,g in esco_clusters.groupby('level_1').level_2:#.value_counts():\n",
    "    level_2_all = sorted(g.value_counts().index.to_list())\n",
    "    esco_first_to_second[name] = level_2_all\n",
    "    for level_id in level_2_all:\n",
    "        esco_second_to_first[level_id] = name\n",
    "    \n",
    "esco_second_to_third={}\n",
    "esco_third_to_second = {}\n",
    "esco_third_to_first = {}\n",
    "for name,g in esco_clusters.groupby('level_2').level_3:#.value_counts():\n",
    "    level_3_all = sorted(g.value_counts().index.to_list())\n",
    "    esco_second_to_third[name] = level_3_all\n",
    "    for level_id in level_3_all:\n",
    "        esco_third_to_second[level_id] = name\n",
    "        esco_third_to_first[level_id] = esco_second_to_first[name]\n",
    "\n",
    "esco_first_to_third = {}\n",
    "for name in esco_first_to_second.keys():\n",
    "    level_2_all = esco_first_to_second[name]\n",
    "    level_3_all = []\n",
    "    for level_id in level_2_all:\n",
    "        level_3_all.append(esco_second_to_third[level_id])\n",
    "    esco_first_to_third[name] = sorted(flatten_lol(level_3_all))\n",
    "\n",
    "print('done')\n",
    "\n",
    "# same but with the levels\n",
    "esco_first_to_second_label={}\n",
    "esco_second_to_first_label={}\n",
    "for name,g in esco_clusters.groupby('label_level_1').label_level_2:#.value_counts():\n",
    "    level_2_all = sorted(g.value_counts().index.to_list())\n",
    "    esco_first_to_second_label[name] = level_2_all\n",
    "    for level_id in level_2_all:\n",
    "        esco_second_to_first_label[level_id] = name\n",
    "    \n",
    "esco_second_to_third_label={}\n",
    "esco_third_to_second_label = {}\n",
    "esco_third_to_first_label = {}\n",
    "for name,g in esco_clusters.groupby('label_level_2').label_level_3:#.value_counts():\n",
    "    level_3_all = sorted(g.value_counts().index.to_list())\n",
    "    esco_second_to_third_label[name] = level_3_all\n",
    "    for level_id in level_3_all:\n",
    "        esco_third_to_second_label[level_id] = name\n",
    "        esco_third_to_first_label[level_id] = esco_second_to_first_label[name]\n",
    "\n",
    "esco_first_to_third_label = {}\n",
    "for name in esco_first_to_second_label.keys():\n",
    "    level_2_all = esco_first_to_second_label[name]\n",
    "    level_3_all = []\n",
    "    for level_id in level_2_all:\n",
    "        level_3_all.append(esco_second_to_third_label[level_id])\n",
    "    esco_first_to_third_label[name] = sorted(flatten_lol(level_3_all))\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['profession_soc_code_1'] = resample_soc_to_n_digits_df(\n",
    "    data_df.profession_soc_code_value, n=1)\n",
    "\n",
    "data_df['profession_soc_code_2'] = resample_soc_to_n_digits_df(\n",
    "    data_df.profession_soc_code_value, n=2)\n",
    "\n",
    "data_df['profession_soc_code_3'] = resample_soc_to_n_digits_df(\n",
    "    data_df.profession_soc_code_value, n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of SOC x skill category crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the skill categories needed for selected occupations\n",
    "level = 3\n",
    "for soccode in [2136.0, 2231.0,231.0]:\n",
    "    k = {2136.0: f'4d_by_level_{level}', 2231.0: f'4d_by_level_{level}', 231.0: f'3d_by_level_{level}'}[soccode]\n",
    "    print(f\"Breakdown at level {level} for occupation '{socnames_dict[soccode]}'\")\n",
    "    labels_to_use = {2: cluster_labels_2, 3: cluster_labels_3}[level]\n",
    "    print(soc_by_clusters_norm['2019'][k].rename(lambda x: labels_to_use.loc[x].label\n",
    "                                                    )[soccode].sort_values(ascending=False).head(10)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEFIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select durations less than 55 days\n",
    "duration_only = deepcopy(data_df.duration)\n",
    "median_duration = duration_only.median()\n",
    "duration_minus = median_duration - 7\n",
    "duration_plus = median_duration + 7\n",
    "duration_only = duration_only[duration_only<55]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of duration, with median and +-7 days around the median\n",
    "with sns.plotting_context('talk'):\n",
    "    sns.set_style('white')\n",
    "    plt.figure(figsize = (9,5))\n",
    "    plt.hist(duration_only, bins = 55, color = 'k')\n",
    "    plt.xlim([0,55])\n",
    "    plt.xlabel('Job advert duration (days)')\n",
    "    plt.ylabel('Number of job adverts')\n",
    "    plt.plot([median_duration,median_duration],[0,1450000],color= nesta_colours[3], linewidth = 3)\n",
    "    plt.plot([duration_minus,duration_minus],[0,1450000], '--', color= nesta_colours[3], linewidth = 3)\n",
    "    plt.plot([duration_plus,duration_plus],[0,1450000], '--', color= nesta_colours[3], linewidth = 3)\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(f\"{output_folder}/duration_histogram_with_median.png\")\n",
    "        plt.savefig(f\"{output_folder}/duration_histogram_with_median.svg\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many job adverts within 7 days from the median?\n",
    "m = data_df['duration'].median()\n",
    "sd = 7#data_df['duration'].std()\n",
    "\n",
    "A = ((data_df['duration']<=m+sd) & (data_df['duration']>=m-sd)).mean()\n",
    "print(f\"Percentage of jobs with durations within 7 days from the median: {A*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the duration field against other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of the dataset with durations less than 100\n",
    "good_durations = data_df[data_df.duration<100][['date', 'duration','profession_soc_code_value',\n",
    "       'final_sic_letter', 'region_label', 'ttwa11cd', 'top_cluster', 'job_title_processed', \n",
    "        'profession_soc_code_1', 'profession_soc_code_2','profession_soc_code_3']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration distribution by 1-digit SOC code\n",
    "sns.boxplot(x = 'profession_soc_code_1', y = 'duration', data= good_durations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration distribution by region\n",
    "f = plt.figure(figsize = (14,7))\n",
    "with sns.plotting_context('talk'):\n",
    "    sns.boxplot(x = 'region_label', y = 'duration',\n",
    "            data= good_durations,\n",
    "                ax = f.gca(),\n",
    "               palette = sns.color_palette())\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration distribution by region and 1-digit SOC code\n",
    "f = plt.figure(figsize = (10,5))\n",
    "with sns.plotting_context('talk'):\n",
    "    sns.boxplot(hue = 'region_label', y = 'duration', x = 'profession_soc_code_1',\n",
    "            data= good_durations, ax = f.gca(),\n",
    "               palette = sns.color_palette())\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration distribution by sector\n",
    "f = plt.figure(figsize = (15,7))\n",
    "sns.boxplot(x = 'final_sic_letter', y = 'duration', data= good_durations, ax=f.gca())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration distribution by skill category\n",
    "duration_by_cluster = good_durations.groupby('top_cluster').duration.agg(['mean','median','count'])\n",
    "duration_by_cluster.rename(columns = {'mean': 'mean_duration', 'median': 'median_duration'})\n",
    "\n",
    "duration_by_cluster = duration_by_cluster[duration_by_cluster['count']>5000]\n",
    "\n",
    "printdf(duration_by_cluster.sort_values('mean').head(15).rename(lambda x: cluster_labels_3.loc[x].label))\n",
    "\n",
    "printdf(duration_by_cluster.sort_values('median').head(15).rename(lambda x: cluster_labels_3.loc[x].label))\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "printdf(duration_by_cluster.sort_values('mean', ascending=False).head(15).rename(lambda x: \n",
    "                                                                            cluster_labels_3.loc[x].label))\n",
    "\n",
    "printdf(duration_by_cluster.sort_values('median', ascending=False).head(15).rename(lambda x: \n",
    "                                                                            cluster_labels_3.loc[x].label))\n",
    "\n",
    "duration_by_cluster = duration_by_cluster.sort_values('median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution of duration by skill category\n",
    "clusters_to_show = duration_by_cluster[(duration_by_cluster['median']<29) |\n",
    "                                      (duration_by_cluster['median']>33)]\n",
    "with sns.plotting_context('talk'):\n",
    "    f = plt.figure(figsize = (8,7))\n",
    "    sns.boxplot(x = 'top_cluster', y = 'duration', data= good_durations, ax = f.gca(),\n",
    "               order = clusters_to_show.index.tolist())\n",
    "\n",
    "    tmp = f.gca().get_xticklabels()\n",
    "    f.gca().set_xticklabels([cluster_labels_3.loc[float(x.get_text())].label for x in tmp])\n",
    "    _ = plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the median duration for each occupation\n",
    "duration_by_soc = data_df.groupby('profession_soc_code_value')['duration'].median()\n",
    "duration_by_soc = pd.DataFrame(duration_by_soc.values,\n",
    "                               index=duration_by_soc.index, columns=['median_duration'])\n",
    "\n",
    "# group by first digit and show distribution\n",
    "duration_by_soc['soc_1'] = pd.DataFrame(duration_by_soc).index.map(\n",
    "    lambda x: resample_soc_to_n_digits(x,n=1))\n",
    "sns.boxplot(x='soc_1', y = 'median_duration' , data=duration_by_soc)\n",
    "\n",
    "# show the bottom and top occupations\n",
    "print('Shortest durations')\n",
    "printdf(duration_by_soc.sort_values(by='median_duration').head(5).rename(socnames_dict))\n",
    "\n",
    "print('Longest durations')\n",
    "printdf(duration_by_soc.sort_values(by='median_duration', ascending=False).head(5).rename(socnames_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durations by job title (most common ones)\n",
    "top_titles= data_df.job_title_processed.value_counts().iloc[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_by_title_sector = {}\n",
    "duration_by_title = {}\n",
    "timer.start_task()\n",
    "for ix,t in enumerate(top_titles.index):\n",
    "    tmp = data_df[data_df.job_title_processed == t][['duration','final_sic_letter']]\n",
    "    tmp_g = tmp.groupby('final_sic_letter').duration.agg([\"median\",\"count\"]).reset_index()\n",
    "    tmp_g['proportion'] = tmp_g['count']/tmp_g['count'].sum()\n",
    "    duration_by_title[t] = tmp.duration.median()\n",
    "    duration_by_title_sector[t]= tmp_g\n",
    "    if ix%10==9:\n",
    "        timer.end_task()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(duration_by_title.values(), index= duration_by_title.keys(), columns = ['median_duration']\n",
    "#            ).plot(kind='hist')\n",
    "printdf(pd.DataFrame(duration_by_title.values(), index= duration_by_title.keys(), columns = ['median_duration']\n",
    "            ).sort_values('median_duration', ascending=False).head(12))\n",
    "pd.DataFrame(duration_by_title.values(), index= duration_by_title.keys(), columns = ['median_duration']\n",
    "            ).sort_values('median_duration').head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
