{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this script does:\n",
    "1. Load the full dataset of vacancies and merge with the SOC letters that have been estimated for each vacancy\n",
    "2. Compute the un-weighted monthly stock of online job adverts (OJA) vacancies broken down by SIC\n",
    "3. Compute the per-vacancy weights to align with the ONS vacancy dataset\n",
    "4. Compute the adjusted per-vacancy weights after taking into account adverts with \"uncertain\" SIC letters\n",
    "5. Save the results for future analysis\n",
    "\n",
    "Note on the presence of adverts with \"uncertain\" SIC letters. \n",
    "- First of all, these adverts are assigned the average weight for that month. The same is true for job adverts with SIC codes that are not measured by the ONS vacancy survey. This obviously messes up the total stock count, which is now higher than the ONS one because the overall sum includes vacancies that were not used to compute the per-vacancy weights. However, if we adjust for this, then the absolute levels by SIC from online job adverts will be smaller than the absolute levels from the ONS because the weights have been decreased to take into account the uncertain vacancies. We can't have it both ways I think. However, I decided to prioritise the alignment by 'total stock level' because otherwise breakdowns by other characteristics might be artificially inflated.\n",
    "\n",
    "\n",
    "Suggestions for future work:\n",
    "- Implement the final per-vacancy weight as the weighted average of the monthly weights spanned by each job advert (weighted by how long a vacancy is open in that month).\n",
    "- Instead of taking the median value across weights to use with \"uncertain\" job adverts, use the mean across all sectors, minus \"Mining and Quarrying\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------ DEPENDENCIES AND FUNCTIONS ------------------------\n",
    "\n",
    "# standard imports\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "#import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from pprint import PrettyPrinter\n",
    "#from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import sys\n",
    "import statsmodels as sm\n",
    "from time import time as tt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# custom imports\n",
    "sys.path.append('/Users/stefgarasto/Google Drive/Documents/scripts/utils_stef')\n",
    "from flow_to_stock_funcs import get_stock_breakdown, load_ons_vacancies, \\\n",
    "                                set_month_at_beginning, set_month_at_end, scale_weights_by_total_levels\n",
    "#from importlib import reload\n",
    "#import textkernel_load_utils\n",
    "#reload(textkernel_load_utils)\n",
    "from textkernel_load_utils import tk_params, create_tk_import_dict, read_and_append_chunks, \\\n",
    "                                  load_full_column, data_path, data_folder\n",
    "# note: data_path is the root to where the data is stored, data_folder = {data_path}/data/processed\n",
    "from utils_general import nesta_colours, flatten_lol, sic_letter_to_text, print_elapsed, TaskTimer, printdf\n",
    "\n",
    "pp = PrettyPrinter(indent=4)\n",
    "# Add custom SIC groups\n",
    "sic_letter_to_text['Z'] = 'others'\n",
    "sic_letter_to_text['L_O_S'] = 'personal_and_public_services'\n",
    "sic_letter_to_text['D_E'] = 'utilities'\n",
    "sic_letter_to_text['M_P'] = 'educational_and_professional_activities'\n",
    "sic_letter_to_text['uncertain'] = 'uncertain'\n",
    "\n",
    "# NOTE: change to local results folder\n",
    "res_folder = '/Users/stefgarasto/Local-Data/textkernel/results/flow_to_stock'\n",
    "\n",
    "timer = TaskTimer()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardcoded parameters setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_DURATION_TH = 55 #55\n",
    "TEST_DURATION_TH = 1\n",
    "CONSTANT_DUR = False\n",
    "DURATION_TH = TEST_DURATION_TH if CONSTANT_DUR else FIXED_DURATION_TH\n",
    "START_MONTH = '2015-03'\n",
    "END_MONTH= '2019-11'\n",
    "FIRST_VALID_MONTH = '2015-04'\n",
    "print(f'Duration threshold is {DURATION_TH}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions, parameters and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def norm_series(df_series):\n",
    "    return (df_series - df_series.mean())/df_series.std()\n",
    "\n",
    "#%%\n",
    "def cap_duration(data, duration_th = 55):\n",
    "    data.loc[data.duration>duration_th,'duration'] = duration_th\n",
    "    return data\n",
    "\n",
    "# invert the sic letter to text mapping\n",
    "sic_text_to_letter = {v: k for k,v in sic_letter_to_text.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get which TK ID value for sector corresponds to which label\n",
    "def get_map_industry_label_values():\n",
    "    tmp_data = pd.read_csv(os.path.join(data_folder, tk_params.file_name_template.format(0)), \n",
    "                           compression='gzip',\n",
    "                encoding = 'utf-8',usecols = ['organization_industry_label','organization_industry_value'])\n",
    "    map_label2value = {}\n",
    "    map_value2label = {}\n",
    "    for name,g in tmp_data.groupby('organization_industry_value'):\n",
    "        map_value2label[name] = g.organization_industry_label.value_counts().index.values[0]\n",
    "        map_label2value[map_value2label[name]] = name\n",
    "    return map_label2value, map_value2label\n",
    "\n",
    "# create the maps\n",
    "map_label2value, map_value2label = get_map_industry_label_values()\n",
    "\n",
    "map_label2value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# [TEST] Create models for the duration field from data\n",
    "def best_fit_distribution(data, bins=200, ax=None):\n",
    "    \"\"\"Model data by finding best fit distribution to data\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- duration data to model\n",
    "    bins -- number of bins to use to discretise the data\n",
    "    ax -- ax of figure where to plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get histogram of original data\n",
    "    y, x = np.histogram(data, density= True, bins = bins)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "    plt.close()\n",
    "    # MLE fit\n",
    "    all_params = {}\n",
    "    \n",
    "    # fit geometric distribution\n",
    "    dist_name = 'geometric'\n",
    "    all_params[dist_name] = {}\n",
    "    all_params[dist_name]['params'] = 1/np.mean(data)\n",
    "    # generate sample\n",
    "    pdf = st.geom.pmf(x, all_params[dist_name]['params'])\n",
    "    all_params[dist_name]['sse'] = np.sum(np.power(y - pdf, 2.0))\n",
    "    \n",
    "    # poisson\n",
    "    dist_name = 'poisson'\n",
    "    all_params[dist_name] = {}\n",
    "    all_params[dist_name]['params'] = np.mean(data)\n",
    "    # generate sample\n",
    "    pdf = st.poisson.pmf(x, all_params[dist_name]['params'])\n",
    "    all_params[dist_name]['sse'] = np.sum(np.power(y - pdf, 2.0))\n",
    "    \n",
    "    # Other distributions to fit\n",
    "    DIST_DICT = {'levy': st.levy, 'gamma': st.gamma, 'invgamma': st.invgamma,\n",
    "                 'chi_squared': st.chi2, 'beta1': st.beta, 'exponential': st.expon}\n",
    "    \n",
    "    # Fit other distributions via MLE \n",
    "    for dist_name in DIST_DICT.keys():\n",
    "        print(dist_name)\n",
    "        all_params[dist_name] = {}\n",
    "        all_params[dist_name]['params'] = DIST_DICT[dist_name].fit(data)\n",
    "        params= all_params[dist_name]['params']\n",
    "        # generate sample\n",
    "        if len(params)==4:\n",
    "            pdf = DIST_DICT[dist_name].pdf(x, params[0],\n",
    "                                           params[1],\n",
    "                                           loc=params[2], \n",
    "                                           scale=params[3])\n",
    "        elif len(params)==3:\n",
    "            pdf = DIST_DICT[dist_name].pdf(x, params[0],\n",
    "                                           loc= params[1],\n",
    "                                           scale=params[2])\n",
    "        else:\n",
    "            pdf = DIST_DICT[dist_name].pdf(x, loc= params[0],\n",
    "                                           scale= params[1])\n",
    "        all_params[dist_name]['sse'] = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "    # beta - after normalisation\n",
    "    dist_name = 'beta2'\n",
    "    loc = min(data) - 1e-5\n",
    "    scale = max(data) - loc + .1\n",
    "    all_params[dist_name] = {}\n",
    "    all_params[dist_name]['params'] = st.beta.fit(data, floc=loc, fscale= scale)\n",
    "    # generate sample\n",
    "    pdf = st.beta.pdf(x, a= all_params[dist_name]['params'][0],\n",
    "                         b=all_params[dist_name]['params'][1],\n",
    "                         loc=all_params[dist_name]['params'][2], \n",
    "                         scale=all_params[dist_name]['params'][3])\n",
    "    all_params[dist_name]['sse'] = np.sum(np.power(y - pdf, 2.0))\n",
    "    \n",
    "    return all_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def get_top_month(x):\n",
    "    \"\"\"Get month in which a vacancy is most active based on a string \n",
    "    listing all the months in which the vacancy is active\n",
    "    \"\"\"\n",
    "    if isinstance(x,str):\n",
    "        x = x.split(';')[1:]\n",
    "    else:\n",
    "        x = x.active_months.split(';')[1:]\n",
    "    months = [t.split(': ')[0].split(' ')[1] for t in x]\n",
    "    durations = [t.split(': ')[1] for t in x]\n",
    "    # if multiple maxes it'll return the first one, which seems reasonable\n",
    "    if len(durations):\n",
    "        best_idx = np.argmax(durations)\n",
    "        return months[best_idx]#, durations[best_idx]\n",
    "    else:\n",
    "        return 'oob'\n",
    "\n",
    "def get_top_duration(x):\n",
    "    \"\"\"Get the amount of time a vacancy is active in its top month\n",
    "    based on a string listing all the months in which the vacancy is active\n",
    "    with respective durations\n",
    "    \"\"\"\n",
    "    if isinstance(x,str):\n",
    "        if x == 'oob':\n",
    "            return 0\n",
    "        x = x.split(';')[1:]\n",
    "    else:\n",
    "        if x.active_months == 'oob':\n",
    "            return 0\n",
    "        x = x.active_months.split(';')[1:]\n",
    "    months = [t.split(': ')[0] for t in x]\n",
    "    durations = [t.split(': ')[1] for t in x]\n",
    "    # if multiple maxes it'll return the first one, which seems reasonable\n",
    "    best_idx = np.argmax(durations)\n",
    "    return durations[best_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def twin_plot(ons_data,ojv_data,xlims = [pd.to_datetime(START_MONTH + '-01'),\n",
    "                                         pd.to_datetime(END_MONTH + '-01')]):\n",
    "    \n",
    "    \"\"\" Plot two timeseries on same axis (ONS vacancies and (un-)weighted stock)\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('date (year-month)')\n",
    "    ax1.set_ylabel('ONS vacancy stock', color=color)\n",
    "    ax1.plot(ons_data, 'x-', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('OJA vacancy stock', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(ojv_data, 'o-', color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.plot(raw_jvs) #norm_series(raw_jvs))\n",
    "    #plt.plot(norm_series(stock_month1)) #df_stock))\n",
    "    plt.xlim(xlims[0],xlims[1])\n",
    "    fig.tight_layout()\n",
    "    return fig, ax1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online vacancy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration, start date, soc code and organisation name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USUAL_LOADING = False\n",
    "SAVE_DF = False\n",
    "#------------------- START OF MAIN SCRIPT --------------------------------------------\n",
    "#%\n",
    "print(tk_params)\n",
    "\n",
    "# define names of files to load\n",
    "N_to_load = tk_params.N_files\n",
    "dfilename = os.path.join(data_folder,tk_params.file_name_full)\n",
    "indices_to_load = np.random.permutation(tk_params.N_files)[:N_to_load]\n",
    "dfilenames = [os.path.join(data_folder,tk_params.file_name_template.format(i)) for i in\n",
    "              indices_to_load]\n",
    "#              np.random.randint(low=0,high=40,size=2)]\n",
    "import_dict, dates_to_parse = create_tk_import_dict()\n",
    "#dimport_dict, dates_to_parse = create_tk_import_dict(DASK_DIRECT=True)\n",
    "\n",
    "\n",
    "cols_to_load = ['job_id',\n",
    "'date','duration',\n",
    "'expiration_date',\n",
    "'posting_id',\n",
    "'profession_soc_code_value',\n",
    "'organization_name',\n",
    "'organization_industry_value']\n",
    "\n",
    "import_dict_new = {}\n",
    "for col in cols_to_load: #import_dict.keys():\n",
    "    if col in import_dict.keys():\n",
    "        import_dict_new[col]= import_dict[col]\n",
    "    else:\n",
    "        import_dict_new[col]= 'object'\n",
    "dates_to_parse_new = [t for t in dates_to_parse if t in cols_to_load]\n",
    "    \n",
    "if USUAL_LOADING:    \n",
    "    # Load only the relevant columns from each data chunk and then join them together\n",
    "    timer.start_task('loading relevant columns from full data')\n",
    "    filename_chunks = os.path.join(data_folder,tk_params.file_name_template)\n",
    "    data_df = read_and_append_chunks(filename_chunks, range(N_to_load), #indices_to_load,\n",
    "                import_dict_new, dates_to_parse_new, col_to_load = cols_to_load)\n",
    "        \n",
    "    timer.end_task()\n",
    "    \n",
    "    # compute the length once and save it\n",
    "    len_data = len(data_df)\n",
    "    # rename the index in case you need it\n",
    "    #data_df = data_df.map_partitions(dask_rename_index,'index')\n",
    "    #duration_backup = data_df.duration.copy()\n",
    "    if SAVE_DF:\n",
    "        # Save to disk for faster future loading\n",
    "        data_df.to_csv(\n",
    "            f'{data_path}/data/interim/interim_duration_df2.gz',\n",
    "            encoding='utf-8',compression='gzip',index=False)\n",
    "else:\n",
    "    # Load from disk - this version of the datatset will have been saved before with only the relevant columns\n",
    "    timer.start_task('Loading reduced dataframe from disk')\n",
    "    data_df = pd.read_csv(\n",
    "        f'{data_path}/data/interim/interim_duration_df2.gz',\n",
    "        encoding='utf-8',compression='gzip', dtype = import_dict, \n",
    "        parse_dates = dates_to_parse, infer_datetime_format = True)\n",
    "    timer.end_task()\n",
    "\n",
    "\n",
    "#%% get beginning and ending of the collection period\n",
    "first_date = data_df.date.min()#.compute()\n",
    "last_date = data_df.date.max()#'2019-10-31' #data_df.date.max().compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {len(data_df)}\")\n",
    "data_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# reset index because at the moments it starts again after loading each chunk\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "# turn date into date time\n",
    "data_df['date'] = pd.to_datetime(data_df.date)\n",
    "data_df = data_df.sort_values(by = 'date')\n",
    "\n",
    "base_columns = data_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-vacancy SIC code\n",
    "\n",
    "Note:\n",
    "We made an algorithm that assigned a SIC code to each vacancy based on a combination of methods (see e.g. the script \"consolidate_sic_matches\"). The main part of the algorithm was run separately and produced a set of candindates SIC per vacancy using the job_id as the unique identifier. Since our dataset has been deduplicated, the job_id is supposed to be unique. However, after running the algorithm we realised that the job_id was not actually unique (the posting_id is). \n",
    "\n",
    "There was no time to re-run the algorithm and as long as the same SIC is assigned to vacancies that have the same set of characteristics used to identify that SIC code (e.g. SOC and TK's own organization industry), it shouldn't matter. The one thing to be careful about is that when joining to the main dataframe, the dataframe with the sic codes should have unique rows or the join creates \"new\" vacancies.\n",
    "\n",
    "For the future, the recommendation would be to re-run the algorithm using posting_id as the unique identifier.\n",
    "\n",
    "Also, it might seem weird that one dataset loads ORGANIZATION_INDUSTRY_VALUE and the other one ORGANIZATION_INDUSTRY_LABEL and then this are converted to match. This is because re-loading both datasets from chunks would take longer than loading the cached version with this discrepancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SIC letters column\n",
    "timer.start_task('Loading results from SIC matching')\n",
    "data_sic = pd.read_csv(f\"{data_path}/data/aux/job_id_and_final_sic_letter.gz\",\n",
    "                          encoding ='utf-8', compression = 'gzip', usecols = ['job_id', 'profession_soc_code_value',\n",
    "                                                                                      'clean_organization_name',\n",
    "                                                                                      'final_sic_letter'],\n",
    "                          dtype= {'job_id': object, 'clean_organization_name': str,\n",
    "                                 'final_sic_letter': 'category',# 'helper_sic_letter': 'category',\n",
    "                                 'profession_soc_code_value': 'category'})\n",
    "timer.end_task()\n",
    "print(f\"Number of rows: {len(data_sic)}\")\n",
    "data_sic.head()\n",
    "# note, there are more rows because adverts not tagged with \"en\" are included\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other information to better merge final SIC letter with original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.start_task('Load full data with SIC codes')\n",
    "DATA_PATH_SIC = f\"{data_path}/data/aux/sic_code_assigned_compressed\"\n",
    "#DATA_PATH0 = (\"/Users/stefgarasto/Local-Data/textkernel/data/\"\n",
    "#              \"sic_code_assigned_compressed_missing_sic_letters\")\n",
    "FILE_NAME = \"sic_code_assigned_full_jobs_200330\"\n",
    "data = []\n",
    "for i in range(435):\n",
    "    loaded_df = pd.read_csv(f\"{DATA_PATH_SIC}/{FILE_NAME}_{i}.gz\", compression = 'gzip',\n",
    "                           usecols = ['job_id',#'profession_soc_code_value','clean_organization_name',\n",
    "                                      'organization_industry_label','fuzzy_match_sic_letter'])\n",
    "    if 'Unnamed: 0' in loaded_df.columns:\n",
    "        loaded_df = loaded_df.drop('Unnamed: 0', axis = 1)\n",
    "    data.append(loaded_df)\n",
    "\n",
    "full_data_sic = pd.concat(data)\n",
    "full_data_sic = full_data_sic.reset_index(drop= True)\n",
    "data = None\n",
    "timer.end_task()\n",
    "\n",
    "printdf(full_data_sic.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine dataframes with information on SIC  codes\n",
    "I don't need to perform a join in this specific case - I know the two dataframes were loaded and saved with rows in the same order. However, if in the future this is done with a different dataset, one needs to be careful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra column about organization industry to improve the merging with the main dataframe\n",
    "assert(data_sic.iloc[:10].job_id.tolist() == full_data_sic.iloc[:10].job_id.tolist())\n",
    "\n",
    "timer.start_task('adding column to sic dataframe')\n",
    "data_sic = data_sic.assign(organization_industry_label = \n",
    "                            full_data_sic.reset_index().organization_industry_label)\n",
    "# If a second column is needed\n",
    "#data_sic['fuzzy_match_sic_letter'] = full_data_sic.reset_index().fuzzy_match_sic_letter\n",
    "timer.end_task()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change from oganization_type_label to organization_type_value\n",
    "timer.start_task('Going from label to value for the organization_type variable')\n",
    "# crosswalk to organization_industry_value\n",
    "data_sic['organization_industry_value'] = data_sic.organization_industry_label.map(lambda x:\n",
    "                                                                                   map_label2value[x])\n",
    "timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One dataframe is not needed anymore\n",
    "full_data_sic = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the duplicate rows: these are rows with the same job_id, same soc_code, same organization_type and, therefore,\n",
    "# same SIC. These rows should be dropped so that the merge doesn't create \"new\" job adverts.\n",
    "timer.start_task('Get duplicated rows from the SIC dataframe')\n",
    "duplicated_sic = data_sic.duplicated(subset = ['job_id','profession_soc_code_value','organization_industry_value'],\n",
    "                                     keep = False)\n",
    "duplicated_sic_first = data_sic.duplicated(\n",
    "                                subset = ['job_id','profession_soc_code_value','organization_industry_value'],\n",
    "                                     keep = 'first')\n",
    "\n",
    "timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of duplicated rows in the dataframe with SIC codes: {duplicated_sic.sum()}')\n",
    "gs = data_sic[duplicated_sic].groupby(by = \n",
    "                        ['job_id','profession_soc_code_value','organization_industry_value'])\n",
    "    \n",
    "# check how many duplicates have mismatched sics\n",
    "A = []\n",
    "for name, g in gs:\n",
    "    A.append(len(set(g.final_sic_letter.values)))\n",
    "\n",
    "counter_A = Counter(A)\n",
    "print(f'Number of duplicated rows with mismatched SIC codes: {counter_A[2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sic.columns, data_df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the SOC code to float in preparation for the merging\n",
    "data_sic.profession_soc_code_value = data_sic.profession_soc_code_value.map(lambda x: float(x))\n",
    "#data_sic.profession_soc_code_value.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes of online job adverts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.start_task('merging data frames')\n",
    "old_len_data = len(data_df)\n",
    "keep_data_sample = deepcopy(data_df.iloc[100:105])\n",
    "#merged_data \n",
    "data_df = data_df.merge(data_sic[~duplicated_sic_first].reset_index()[\n",
    "    ['job_id','profession_soc_code_value','organization_industry_value', \n",
    "     'clean_organization_name','final_sic_letter']], \n",
    "                            on = ['job_id','profession_soc_code_value','organization_industry_value'],\n",
    "                           how= 'left', indicator = True)\n",
    "timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows before joining: {old_len_data}\")\n",
    "print(f\"Number of rows after joining: {len(data_df)}\")\n",
    "# Check that everything stayed the same\n",
    "printdf(keep_data_sample)\n",
    "printdf(data_df.iloc[100:105])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the merging didn't create any new row: count for 'right_only' should be 0\n",
    "data_df['_merge'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composition of job adverts by SIC letter\n",
    "data_df.final_sic_letter.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONS data on vacancies\n",
    "raw_jvs_full, jvs_sic_letters = load_ons_vacancies(f\"{data_path}/data\")\n",
    "# Change all the columns names\n",
    "raw_jvs_full = raw_jvs_full.rename(columns = {t: jvs_sic_letters.loc[t] for t in jvs_sic_letters.index})\n",
    "\n",
    "printdf(raw_jvs_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that are not needed\n",
    "raw_jvs = raw_jvs_full.drop(['D','E', 'G45', 'G46', 'G47', 'L', 'M', 'O', 'P', 'S', 'G46_47'] , axis = 1)\n",
    "\n",
    "printdf(raw_jvs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: A, T and uncertain will all have to get the average weight across month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and processing of the duration field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEFIGS = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick analysis of duration field\n",
    "full_median_duration = data_df.duration.median()\n",
    "tmp = data_df.duration.dropna().value_counts().sort_index()\n",
    "plt.plot(tmp.cumsum()/tmp.sum()*100)\n",
    "plt.xlim([0,100])\n",
    "plt.plot(DURATION_TH,tmp.cumsum()[DURATION_TH]/tmp.sum()*100,'x')\n",
    "plt.xlabel('Duration value')\n",
    "plt.ylabel('Proportion of jobs')\n",
    "print((f'Percentage of filtered job adverts with duration within limit ({DURATION_TH} days is the threshold),'\n",
    "       ' among the ones with a not null duration field:'\n",
    "       f' {tmp.cumsum()[DURATION_TH]/tmp.sum()*100:.2f}%'))\n",
    "if SAVEFIGS:\n",
    "    plt.savefig(f\"{res_folder}/cumulative_sum_of_durations.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# If we want to try to model distributions outliers, we first need to fit the duration distribution \n",
    "# However, we didn't find a good theoretical distribution, so we're not doing this at the moment\n",
    "# ATM, we just use a duration threshold that has been chosen manually\n",
    "if False:\n",
    "    #%\n",
    "    # durations to fit\n",
    "    duration_to_fit = data_df[good_counts].duration.dropna() #[good_counts].duration.dropna()\n",
    "    # remove the zeros\n",
    "    duration_to_fit = duration_to_fit[duration_to_fit>0]\n",
    "    # Plot for comparison\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.distplot(duration_to_fit)\n",
    "    # Save plot limits\n",
    "    dataYLim = ax.get_ylim()\n",
    "    \n",
    "    # Find best fit distribution\n",
    "    all_fit_params = best_fit_distribution(duration_to_fit, 200, ax)\n",
    "\n",
    "    #%\n",
    "    DIST_DICT = {'levy': st.levy, 'gamma': st.gamma, 'invgamma': st.invgamma,\n",
    "                     'chi_squared': st.chi2, 'beta1': st.beta, 'beta2': st.beta,\n",
    "                     'exponential': st.expon, 'geometric': st.geom,\n",
    "                     'poisson': st.poisson}\n",
    "\n",
    "    #% plot all the distributions\n",
    "    plt.figure()\n",
    "    sns.distplot(duration_to_fit[duration_to_fit<200])\n",
    "    x = np.arange(1,200)#1353)\n",
    "    legend_list = []\n",
    "    result_fit = pd.DataFrame.from_dict(all_fit_params, orient = 'index')\n",
    "    result_fit= result_fit.sort_values('sse')\n",
    "    print(result_fit)\n",
    "    for k in result_fit.index:\n",
    "        params = all_fit_params[k]['params']\n",
    "        if isinstance(params,(np.float,np.float32)):\n",
    "            pdf = DIST_DICT[k].pmf(x, params)\n",
    "        elif len(params)==2:\n",
    "            pdf = DIST_DICT[k].pdf(x, loc = params[-2], scale = params[-1])\n",
    "        elif len(params)==3:\n",
    "            pdf = DIST_DICT[k].pdf(x, params[0], loc = params[-2], scale = params[-1])\n",
    "        elif len(params)==4:\n",
    "            pdf = DIST_DICT[k].pdf(x, a= params[0], b= params[1], loc = params[-2], scale = params[-1])\n",
    "        # plot pdf\n",
    "        if pdf.max()<.06:\n",
    "            plt.plot(pdf),plt.xlim([0,200])\n",
    "            legend_list.append(k)\n",
    "    plt.legend(legend_list + ['data'])\n",
    "    \n",
    "    #%\n",
    "    # take the best fit distribution to determine after which value do advert life\n",
    "    # spans become so low probability that they may be considered unreliable\n",
    "    select_dist = result_fit.iloc[0]\n",
    "    select_pdf= DIST_DICT[select_dist.name].pdf(x, *select_dist.params)\n",
    "    select_cdf= DIST_DICT[select_dist.name].cdf(x, *select_dist.params)\n",
    "    # find very unlikely advert life spans\n",
    "    # if I were to take .95 I would get a threshold of 89 days - \n",
    "    # .84 is needed to get 55 days, which is consistent with what we know from the cumsum above\n",
    "    duration_th = (select_cdf>.95).argmax()\n",
    "    print(f'Data-driven cut off duration is {duration_th}')\n",
    "    \n",
    "    #%\n",
    "else:\n",
    "    # Just assign the duration threshold that has been manually chosen\n",
    "    duration_th = DURATION_TH\n",
    "    print(f'Cut off duration is {duration_th}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# replace \"bad\" duration values (that is, those that are zeros or higher than the threshold)\n",
    "good_durations = (data_df.duration>0) & (data_df.duration<=duration_th)\n",
    "# take the median from those durations that will not be changed\n",
    "median_duration = data_df[good_durations].duration.median()\n",
    "print(f'Median duration to use is {median_duration}')\n",
    "\n",
    "if CONSTANT_DUR:\n",
    "    data_df['duration_to_use'] = DURATION_TH\n",
    "    print(f'Using constant duration of {DURATION_TH}')\n",
    "else:\n",
    "    data_df['duration_to_use'] = data_df.duration.copy()\n",
    "    # replace 0s and 1s\n",
    "    data_df.loc[data_df.duration_to_use==0,'duration_to_use'] = median_duration\n",
    "    data_df.loc[data_df.duration_to_use>duration_th,'duration_to_use'] = duration_th #median_duration\n",
    "    data_df.duration_to_use = data_df.duration_to_use.fillna(median_duration)\n",
    "    sns.distplot(data_df.duration_to_use)\n",
    "\n",
    "assert(data_df.duration_to_use.isna().sum()==0)\n",
    "print(f'Max duration used in the dataset is {data_df.duration_to_use.max()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(re-)compute end date \n",
    "'''\n",
    "Note that I'm using the convention of removing 1 (even though it means I need to shift the removal day by 1 \n",
    "when computing the stock) because a) it makes sense to have the expiration date rather than the removal date\n",
    "and b) that is how the original expiration date is in the TK dataset\n",
    "'''\n",
    "data_df['end_date'] = data_df.date + pd.to_timedelta(\n",
    "        data_df.duration_to_use - 1, unit='D')\n",
    "\n",
    "# initialise weight column with 1\n",
    "data_df['vacancy_weight']= 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow to stock model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and plot un-weighted monthly stock of vacancies against ons vacancies\n",
    "t0 = tt()\n",
    "# new way\n",
    "stock_per_month, stock_per_day, _, _ = get_stock_breakdown(\n",
    "    data_df, agg_func = 'count', agg_col = 'vacancy_weight', breakdown_col = 'final_sic_letter')\n",
    "\n",
    "print_elapsed(t0,'computing daily and monthly stock')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_per_month.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get which SIC codes are in both stocks (ONS and online job adverts)\n",
    "sic_in_common = sorted(set(data_df.final_sic_letter.value_counts().index).intersection(raw_jvs.columns))\n",
    "print(sic_in_common)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ONS vs OJA stock (on separate scales)\n",
    "_ = twin_plot(1e3*raw_jvs.vacancies, stock_per_month[sic_in_common].sum(axis=1))\n",
    "    \n",
    "#plt.legend(['ONS VS', 'OJV'])\n",
    "print('Starting date for the stock: ', stock_per_month.index.min())\n",
    "if SAVEFIGS:\n",
    "    plt.savefig(f\"{res_folder}/raw_stock_vs_ons_double_axis.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ONS vs OJA stock (on same scale)\n",
    "# Note: this is just indicative and the overall level depends on the exact flow to stock model used,\n",
    "# including the details of the boundary conditions - if OJA levels are lower, it doesn't say anything about the\n",
    "# coverage of Textkernel data, especially because the stock of OJA is divided by a factor of 2 to account for the \n",
    "# fact that the ONS vacancy survey is only open for two weeks per month.\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.plot(stock_per_month.sum(axis=1))\n",
    "plt.plot(raw_jvs.vacancies*1000)\n",
    "plt.xlabel('Date', fontsize = 13)\n",
    "plt.ylabel('Vacancy stock', fontsize = 13)\n",
    "plt.legend(['OJA','ONS'], fontsize = 13) #OJA = online job adverts\n",
    "plt.tight_layout()\n",
    "if SAVEFIGS:\n",
    "    plt.savefig(f\"{res_folder}/raw_stock_vs_ons_single_axis.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot SIC letters separately: this can only be used to understand whether the trends are similar between OJA and ONS\n",
    "for col in sic_in_common: #stock_per_month.columns:\n",
    "    if col == 'V':\n",
    "        continue\n",
    "    _ = twin_plot(1e3*raw_jvs[col], stock_per_month[col])\n",
    "    tmp = np.corrcoef(raw_jvs[col].astype('float'),stock_per_month[col])[0,1]\n",
    "    print((f\"Time series correlation for {sic_letter_to_text[col]} is \"\n",
    "           f\"{tmp:.3f}\"))\n",
    "    try:\n",
    "        plt.title(sic_letter_to_text[col].capitalize())\n",
    "    except:\n",
    "        plt.title('others')\n",
    "    if SAVEFIGS:\n",
    "        plt.savefig(f\"{res_folder}/raw_stock_vs_ons_sic_{col}_double_axis.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the time series of the non-assigned stock\n",
    "plt.plot(stock_per_month['uncertain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of jobs without a SIC?\n",
    "(data_df.final_sic_letter == 'uncertain').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full list of sectors and which ones are in common with the ONS data \n",
    "oja_names = sorted(stock_per_month.columns)\n",
    "ons_names = [col+'_ons' for col in sorted(raw_jvs.columns) if col in oja_names]\n",
    "shared_oja_names = [t for t in oja_names if t in raw_jvs.columns]\n",
    "extra_oja_names = [t for t in oja_names if t not in shared_oja_names]\n",
    "shared_oja_names, extra_oja_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the uncertain stock separate\n",
    "stock_per_day_full = stock_per_day.copy() #[['A','T','uncertain']]\n",
    "stock_per_month_full = stock_per_month.copy() #[['A','T','uncertain']]\n",
    "\n",
    "# drop the original column\n",
    "stock_per_day = stock_per_day.drop(axis = 1, labels = extra_oja_names) #['A','T','uncertain'])\n",
    "stock_per_month = stock_per_month.drop(axis = 1, labels = extra_oja_names) #['A','T','uncertain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(stock_per_day<0).sum() #THE STOCK CAN NOT BE NEGATIVE: this should be empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute post-sampling weights to align the two data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------- MONTHLY WEIGHTS ESTIMATES -------------------\n",
    "\n",
    "Compute the ratio between the two stocks per month, from the ONS and from OJA\n",
    "\n",
    "Assign an average weight to the vacancies with uncertain SIC based on their assigned month\n",
    "\n",
    "Assigning per-vacancy weight based on an assigned month and SIC letter\n",
    "\n",
    "Rescale the per-vacancy weight by a monthly factor to increase alignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract needed information for each vacancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all months of interest\n",
    "all_months = pd.date_range(start= START_MONTH, end = END_MONTH, freq = 'M').map(\n",
    "    set_month_at_beginning)\n",
    "\n",
    "# initialise new dataframe - for each vacancy I want to compute the best month\n",
    "# and how long it stays open during that month\n",
    "data_df = data_df.assign(active_months = '')\n",
    "\n",
    "# For each vacancy, get all the months in which it is active and the relative duration\n",
    "for month in tqdm(all_months):\n",
    "    tot_days = month.days_in_month\n",
    "    month_begins = month # - pd.Timedelta(tot_days-1,unit='days')\n",
    "    month_ends = set_month_at_end(month)\n",
    "    # extract all jobs that are active during this month\n",
    "    jobs_starting_now = data_df.date.between(month_begins, month_ends)\n",
    "    jobs_ending_later = ((data_df.date<month_begins) & (\n",
    "                data_df.end_date>month_begins))\n",
    "    valid_jobs = jobs_starting_now | jobs_ending_later\n",
    "\n",
    "    valid_durations = (data_df[valid_jobs].end_date.map(lambda x: \n",
    "        min([x,month_ends])) - data_df[valid_jobs].date.map(lambda x: \n",
    "        max([x,month_begins]))).map(lambda x: (x.days+1)/tot_days)\n",
    "                                                          \n",
    "    #vacancies_per_month.loc[valid_jobs,'active_durations'] = valid_durations\n",
    "    # record the active month\n",
    "    data_df.loc[valid_jobs,'active_months'] = data_df[\n",
    "        valid_jobs].active_months.map(\n",
    "        lambda x: x+ f';month {month.year}-{month.month:02}: ')\n",
    "    # append the durations\n",
    "    data_df.loc[valid_jobs,'active_months'] = data_df[\n",
    "        valid_jobs].active_months + valid_durations.map(lambda x: f'{x:.3f}')\n",
    "        #vacancies_per_month[valid_jobs].tmp_months.map(lambda x: f'{x:.3f}')\n",
    "\n",
    "#assert(len(vacancies_per_month)== len(data_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Add column with best month\n",
    "t0 = tt()\n",
    "data_df['best_month'] = data_df.active_months.map(\n",
    "    get_top_month)\n",
    "\n",
    "#turn all months to beginning of the month timestamp\n",
    "data_df.best_month = pd.to_datetime(data_df.best_month).map(set_month_at_beginning)\n",
    "\n",
    "\n",
    "print_elapsed(t0,'getting the best month')\n",
    "#assert(len(vacancies_per_month) == len(data_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Add column with the duration of a vacancy in its best month\n",
    "t0 = tt()\n",
    "data_df['best_month_duration'] = data_df.active_months.map(\n",
    "    get_top_duration)\n",
    "data_df.best_month_duration = data_df.best_month_duration.astype('float')\n",
    "print_elapsed(t0,'getting how long vacancies are open in their best months')\n",
    "#assert(len(vacancies_per_month) == len(data_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the ratio between the two stocks per month, from the ONS and from OJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% # compute weights by months and SIC\n",
    "# join ONS and OJA data\n",
    "\n",
    "joint_stock = raw_jvs[['vacancies']+shared_oja_names].merge(stock_per_month.copy(), how = 'outer',\n",
    "                            suffixes = ('_ons','_counts'),\n",
    "                            left_index = True, right_index=True)\n",
    "\n",
    "for col,ons_col in zip(shared_oja_names,ons_names):\n",
    "    #print(col,ons_col)\n",
    "    assert(col+'_ons'==ons_col)\n",
    "    #joint_stock = joint_stock.assign(ratio = lambda x: x[ons_name]*1000/x[oja_name])\n",
    "    joint_stock[col+'_weight'] = joint_stock[ons_col]*1000/joint_stock[col+'_counts'].replace(0,pd.NA)\n",
    "\n",
    "joint_stock = joint_stock[sorted(joint_stock.columns)]\n",
    "\n",
    "joint_stock = joint_stock.replace(np.inf, 0)\n",
    "\n",
    "#monthly_weights = pd.Series(1000*raw_jvs.vacancies.values[1:]/stock_month1.job_id.values,\n",
    "#                            index= all_months[1:])\n",
    "#joint_stock.loc[pd.to_datetime('2015-03-01'),'ratio'] = 1\n",
    "for col in shared_oja_names:\n",
    "    joint_stock[col+'_weight'] = joint_stock[col+'_weight'].astype('float')\n",
    "\n",
    "# rename the columns\n",
    "joint_stock = joint_stock.rename(columns = {'vacancies': 'vacancies_ons'})#, \n",
    "#                                            'vacancy_weight': 'stock_per_month_count'})\n",
    "\n",
    "# replace NaN with the neutral weight (which is 1)\n",
    "joint_stock = joint_stock.fillna(1)\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_stock.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign an average weight to the vacancies with uncertain SIC based on their assigned month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the weight for the 'uncertain' category as the median across all SIC codes\n",
    "# I need to replicate them for all the categories of SIC codes that are not present in the ONS survey\n",
    "joint_stock = joint_stock.assign(uncertain_weight = joint_stock[\n",
    "    [col+'_weight' for col in shared_oja_names]].median(axis = 1))\n",
    "'''\n",
    "# In future, suggest replacing with this other line. Results are VERY similar but not identical\n",
    "joint_stock = joint_stock.assign(uncertain_weight = joint_stock[\n",
    "    [col+'_weight' for col in shared_oja_names if not 'B' in col]].mean(axis = 1))\n",
    "'''\n",
    "\n",
    "joint_stock = joint_stock.assign(A_weight = joint_stock.uncertain_weight.values)\n",
    "\n",
    "joint_stock = joint_stock.assign(T_weight = joint_stock.uncertain_weight.values)\n",
    "\n",
    "# rename columns for consistency\n",
    "for col in oja_names:\n",
    "    if col not in shared_oja_names:\n",
    "        joint_stock = joint_stock.rename(columns = {col: col+'_counts'})\n",
    "\n",
    "joint_stock = joint_stock[sorted(joint_stock.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_stock.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example weights for agriculture\n",
    "joint_stock.iloc[:5].A_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the monthly weights with the main dataframe \n",
    "That is, Assigning per-vacancy weight based on an assigned month and SIC letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe for merging, i.e. put it in long form\n",
    "weights_cols = [col for col in joint_stock.columns if 'weight' in col]\n",
    "joint_stock_weights = joint_stock[weights_cols]\n",
    "joint_stock_weights = joint_stock_weights.reset_index()\n",
    "joint_stock_weights = pd.melt(joint_stock_weights, id_vars='month', \n",
    "                              value_vars = weights_cols,\n",
    "                             value_name='vacancy_weight_adj',\n",
    "                             var_name = 'sic_letter')\n",
    "joint_stock_weights = joint_stock_weights.rename(columns = {'month': 'best_month'})\n",
    "joint_stock_weights.sic_letter = joint_stock_weights.sic_letter.map(lambda x: x[:-7])\n",
    "joint_stock_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%\n",
    "# [if needed] remove weights from previous iterations with vacancies per month\n",
    "if 'vacancy_weight_adj' in data_df.columns:\n",
    "    print('removing old iteration of monthly weights')\n",
    "    data_df = data_df.drop(axis = 1, labels = 'vacancy_weight_adj')\n",
    "    #data_df = data_df[keep_columns]\n",
    "\n",
    "\n",
    "#%%\n",
    "# Merge vacancy weights based on sic classification and month\n",
    "timer.start_task('joining new monthly weights')\n",
    "\n",
    "small_df = None\n",
    "# MERGING WEIGHTS WITHIN THE MAIN DATAFRAME\n",
    "data_df = pd.merge(data_df, joint_stock_weights, \n",
    "                               left_on = ['final_sic_letter','best_month'], \n",
    "                               right_on= ['sic_letter','best_month'],\n",
    "                               how = 'left')\n",
    "assert(old_len_data== len(data_df))\n",
    "\n",
    "data_df.vacancy_weight = data_df.vacancy_weight_adj\n",
    "#small_df = small_df.drop(axis = 1, labels = 'monthly_weight')\n",
    "timer.end_task()\n",
    "\n",
    "# multiply by the duration percentage\n",
    "data_df['vacancy_weight_adj'] = data_df.vacancy_weight * data_df.best_month_duration\n",
    "\n",
    "\n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = tt()\n",
    "print(data_df.vacancy_weight.max())\n",
    "print_elapsed(t0,'')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the per-vacancy weight by a monthly factor to increase alignment\n",
    "\n",
    "This function is used to scale the per-vacancy weights used to align the stock of online job vacancies with the stock of vacancies from the ONS survey. For more info see the docstring of the function 'scale_weights_by_total_levels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute the scaled weights\n",
    "new_weights_df = scale_weights_by_total_levels(joint_stock_weights.rename(\n",
    "    columns = {'sic_letter': 'final_sic_letter', 'vacancy_weight_adj': 'vacancy_weight'}), \n",
    "    raw_jvs, stock_per_month_full, sectors_in_common = shared_oja_names)\n",
    "printdf(new_weights_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join up with main dataframe\n",
    "timer.start_task('joining dataframe to add re-scale adjustment weights')\n",
    "data_df = data_df.merge(new_weights_df[['best_month','final_sic_letter','vacancy_weight_new']], \n",
    "                        on = ['best_month','final_sic_letter'], how ='left')\n",
    "timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.rename(columns = {'vacancy_weights_new': 'vacancy_weight_new'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-compute the stock of vacancies and show the results\n",
    "\n",
    "TODO: clean up some of these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% get new daily stock of vacancies\n",
    "timer.start_task('recomputing daily and monthly OJV stock')\n",
    "new_stock_per_month, _, _ , _ = get_stock_breakdown(data_df, agg_func = 'sum', \n",
    "                               agg_col = 'vacancy_weight_new', breakdown_col = 'final_sic_letter')\n",
    "timer.end_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot ONS stock, un-weighted OJA stock, weighted  OJA stock, WITHOUT the uncertain weights - there should be\n",
    "# a drop in level\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(new_stock_per_month[sic_in_common].sum(axis=1))\n",
    "plt.plot(1000*raw_jvs.vacancies,'--')\n",
    "plt.plot(stock_per_month[sic_in_common].sum(axis=1))\n",
    "plt.xlabel('Date', fontsize = 13)\n",
    "plt.ylabel('Vacancy stock', fontsize = 13)\n",
    "plt.legend(['OJV after','ONS', 'OJV before'], fontsize = 13)\n",
    "if SAVEFIGS:\n",
    "    plt.savefig(f\"{res_folder}/adjusted_stock_vs_raw_vs_ons_common_sic_single_axis.svg\")\n",
    "\n",
    "\n",
    "#%% Plot ONS stock, un-weighted OJA stock, weighted  OJA stock, WITH the uncertain weights - now total level should\n",
    "# be the same\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(new_stock_per_month.sum(axis=1))\n",
    "plt.plot(1000*raw_jvs.vacancies,'--')\n",
    "plt.plot(stock_per_month.sum(axis=1))\n",
    "plt.xlabel('Date', fontsize = 13)\n",
    "plt.ylabel('Vacancy stock', fontsize = 13)\n",
    "plt.legend(['OJV after','ONS', 'OJV before'], fontsize = 13)\n",
    "if SAVEFIGS:\n",
    "    plt.savefig(f\"{res_folder}/adjusted_stock_vs_raw_vs_ons_all_sic_single_axis.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot ONS and weighted OJA stock for each SIC letters separately. Again, total levels are likely to be different\n",
    "for col in shared_oja_names: #stock_per_month.columns:\n",
    "    if col in ['A','T','uncertain']:\n",
    "        plt.plot(new_stock_per_month[col])\n",
    "        plt.title(\"Stock of 'uncertain' vacancies\")\n",
    "    else:\n",
    "        #_ = twin_plot(1e3*raw_jvs[col], new_stock_per_month[col])\n",
    "        plt.figure(figsize = (8,8))\n",
    "        plt.plot(new_stock_per_month[col],label = 'OJV after')\n",
    "        plt.plot(1000*raw_jvs[col],'--',label='ONS')\n",
    "        plt.plot(stock_per_month[col],label = 'OJV before')\n",
    "        plt.xlabel('Date', fontsize = 13)\n",
    "        plt.ylabel('Vacancy stock', fontsize = 13)\n",
    "        plt.legend(fontsize = 13)\n",
    "        tmp = np.corrcoef(raw_jvs[col].astype('float'),new_stock_per_month[col])[0,1]\n",
    "        print((f\"Time series correlation for {sic_letter_to_text[col]} is \"\n",
    "               f\"{tmp:.3f}\"))\n",
    "        plt.title(sic_letter_to_text[col])\n",
    "        if SAVEFIGS:\n",
    "            plt.savefig(f\"{res_folder}/adjusted_stock_vs_raw_vs_ons_sic_{col}_single_axis\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the new stock per month with the un-weighted one\n",
    "joint_stock = joint_stock.merge(new_stock_per_month, left_index = True, \n",
    "                                right_index = True)\n",
    "joint_stock = joint_stock.rename(columns = \n",
    "                                 {col: col+'_sum' for col in new_stock_per_month.columns})\n",
    "joint_stock = joint_stock[sorted(joint_stock.columns)]\n",
    "#'vacancy_weight': 'stock_per_month_sum'})\n",
    "joint_stock.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and show the correlation between ONS and OJA stock before (un-weighted) and after (weighted)\n",
    "for col in shared_oja_names: #new_stock_per_month.columns:\n",
    "    if col in ['A','T','uncertain']:\n",
    "        continue\n",
    "    print(f\"Correlation before and after for {col}\")\n",
    "    joint_stock[col+'_ons'] = joint_stock[col+'_ons'].astype('float')\n",
    "    print(joint_stock[[col+'_ons',col+'_counts',col+'_sum']].corr()[col+'_ons'])\n",
    "    print(f\"MSE before and after for {col}\")\n",
    "    mse_before = ((joint_stock[col+'_ons'] - joint_stock[col+'_counts']/1000)**2).sum()\n",
    "    mse_after = ((joint_stock[col+'_ons'] - joint_stock[col+'_sum']/1000)**2).mean()\n",
    "    print(pd.Series([mse_before,mse_after],index = [col + t for t in ['_counts','_sum']]))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "# Might be useful to save results to test and compare different flow-to-stock and re-weighting models\n",
    "SAVE_STOCK = False\n",
    "if SAVE_STOCK:\n",
    "    joint_stock.to_csv((f\"{res_folder}\"\n",
    "                f\"/monthly_stock_by_sic_before_and_after_with_duration_th_at_{DURATION_TH}.csv\"))\n",
    "    # save the old stock\n",
    "    stock_per_month.to_csv((f\"{res_folder}\"\n",
    "                f\"/raw_monthly_stock_by_sic_from_jobs_with_duration_th_at_{DURATION_TH}.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save monthly weights to disk \n",
    "\n",
    "Do this so that we can load and join them with the main dataframe for future analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save per-vacancy weights so that they can be more easily loaded for further analysis\n",
    "data_df[['job_id', 'date', 'duration', 'posting_id',\n",
    "         'profession_soc_code_value', 'organization_industry_value',\n",
    "         'organization_name', 'clean_organization_name',\n",
    "         'final_sic_letter', 'duration_to_use', \n",
    "         'vacancy_weight', 'vacancy_weight_new',\n",
    "         'active_months', 'best_month', 'best_month_duration']].to_csv(\n",
    "    f\"{data_path}/data/interim/interim_job_id_and_vacancy_weights.gz\", \n",
    "    compression = 'gzip', encoding = 'utf-8', index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy cells\n",
    "\n",
    "These functions are either not needed or have been moved to the script \"flow_to_stock_funcs.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def load_ons_vacancies(start_date = '2015-03-01', end_date = '2019-10-31'):\n",
    "    \"\"\" \n",
    "    Load and process ONS monthly, non-seasonally adjusted vacancy data [dataset x06].\n",
    "    This function is tailored to the current grouping of some SIC codes.\n",
    "    \n",
    "    Note: it assumes the dataset has been downloaded and put in the data/aux folder path.\n",
    "    \n",
    "    Arguments:\n",
    "    start_date = first day of month from when to start the time series\n",
    "    end_date = last day of month from when to end the time series\n",
    "    \"\"\"\n",
    "    ons_df = pd.read_excel(f\"{data_path}/data/aux/x06apr20.xls\", sheet_name='Vacancies by industry',\n",
    "                          skiprows = 3)\n",
    "    # keep columns with data and clean the column names\n",
    "    ons_df = ons_df[[col for col in ons_df.columns if not 'Unnamed:' in col]]\n",
    "    cleaned_col_names = {}\n",
    "    for col in ons_df.columns[2:]:\n",
    "        cleaned_col = col.replace('&', 'and').replace('-','').replace(',','').lower()\n",
    "        cleaned_col = ''.join([t for t in cleaned_col if not t.isdigit()])\n",
    "        cleaned_col_names[col] = '_'.join(cleaned_col.split())\n",
    "    # manual adjustment for one column\n",
    "    cleaned_col_names['Manu-    facturing'] = 'manufacturing'\n",
    "    cleaned_col_names['SIC 2007 sections'] = 'month'\n",
    "    cleaned_col_names['All vacancies1 '] = 'vacancies'\n",
    "    ons_df = ons_df.rename(columns = cleaned_col_names)\n",
    "    # extract the row with the letters\n",
    "    sic_letters = ons_df.iloc[0]\n",
    "    # remove empty rows\n",
    "    ons_df = ons_df.loc[(ons_df.month.notna()) & (ons_df.vacancies.notna())]\n",
    "    # join up some industries\n",
    "    ons_df = ons_df.assign(wholesale_retail_motor_trade_and_repair = \n",
    "                           ons_df.motor_trades + ons_df.wholesale + ons_df.retail)\n",
    "    ons_df = ons_df.assign(wholesale_and_retail = ons_df.wholesale + ons_df.retail)\n",
    "    #ons_df = ons_df.assign(others = ons_df.vacancies - ons_df[partial_map_tk2sic.values()].sum(axis=1))\n",
    "    \n",
    "    ons_df = ons_df.assign(education_and_professional_activities = ons_df.education + \n",
    "                                                        ons_df.professional_scientific_and_technical_activities)\n",
    "    ons_df = ons_df.assign(utilities = ons_df.electricity_gas_steam_and_air_conditioning_supply + \n",
    "                           ons_df.water_supply_sewerage_waste_and_remediation_activities)\n",
    "    ons_df = ons_df.assign(personal_and_public_services = ons_df.real_estate_activities + \n",
    "                                                    ons_df['public_admin_and_defence;_compulsory_social_security'] +\n",
    "                                                    ons_df.other_service_activities)\n",
    "    sic_letters.loc['wholesale_retail_motor_trade_and_repair'] = 'G'\n",
    "    sic_letters.loc['wholesale_and_retail'] = 'G46_47'\n",
    "    sic_letters.loc['education_and_professional_activities'] = 'M_P'\n",
    "    sic_letters.loc['utilities'] = 'D_E'\n",
    "    sic_letters.loc['personal_and_public_services'] = 'L_O_S'\n",
    "    sic_letters.loc['others'] = 'Z'\n",
    "    sic_letters.loc['vacancies'] = 'vacancies'\n",
    "    #\n",
    "    ons_df.month = pd.to_datetime(ons_df.month)\n",
    "    # only need vacancies within a certain period\n",
    "    ons_df = ons_df[(ons_df.month>=pd.to_datetime(start_date)) & \n",
    "                    (ons_df.month<=pd.to_datetime(end_date))]\n",
    "    ons_df = ons_df.set_index('month')\n",
    "    return ons_df, sic_letters\n",
    "'''\n",
    "1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------------------------------------------------------------------------\n",
    "#              Main functions to convert from flow to stock\n",
    "#% --------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "#%% Moved to separate file\n",
    "def get_stock_v0(data, agg_func = 'count', agg_col = 'vacancy_weight', BOUNDARY = None):\n",
    "                    #start_day = '2015-03-01', end_day = '2019-10-31',\n",
    "#                    GET_END_DATE = False, BOUNDARY = None):\n",
    "    \"\"\"Compute the daily stock of vacancies via difference of cumulative sums.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- dataframe with online job vacancies. Need to have \"date\", \"end_date\" and agg_col columns\n",
    "    agg_func: whether to count the vacancies or to sum the weights\n",
    "    agg_col -- reference column to aggregate (usually column with per-vacancy weights)\n",
    "    BOUNDARY -- what to do wrt boundary conditions (start and end month)\n",
    "\n",
    "    # obs: taking the difference of the cumulative sum is the same as doing this:\n",
    "    df_stock2 = pd.DataFrame(columns = ['open_vacancies'], \n",
    "                             index= pd.date_range(start='2015-03-13',end='2019-10-31',freq='D'))\n",
    "    df_stock2.open_vacancies = 0\n",
    "    for reference_day in tqdm(df_stock.index):\n",
    "        df_stock2.loc[reference_day] = ((data.date<=reference_day) &\n",
    "                               (data.end_date>reference_day)).sum()\n",
    "    TO CHECK: Why is it different from the function below?\n",
    "    \"\"\"\n",
    "    \n",
    "    start_day = data.date.min()\n",
    "    end_day = data.date.max()\n",
    "    \n",
    "    if agg_func == 'count':\n",
    "        df_start = data.groupby('date').count()[agg_col].to_frame()#.reset_index()\n",
    "        df_end = data.groupby('end_date').count()[agg_col].to_frame()#.reset_index()\n",
    "    elif agg_func == 'sum':\n",
    "        df_start = data.groupby('date').sum()[agg_col].to_frame()#.reset_index()\n",
    "        df_end = data.groupby('end_date').sum()[agg_col].to_frame()#.reset_index()\n",
    "    else:\n",
    "        print('Wrong aggregate function')\n",
    "        assert(agg_func in ['count','sum'])\n",
    "    \n",
    "    #df_start = df_start.set_index('date').resample('1D').mean().fillna(0)\n",
    "    #df_end = df_end.set_index('end_date').resample('1D').mean().fillna(0)\n",
    "    # shift the end date by one, since vacancies disappear the day after their expiration date\n",
    "    df_end = df_end.shift(1)\n",
    "\n",
    "    df_start = df_start.reindex(pd.date_range(start=start_day,\n",
    "                                    end=end_day,freq='D'), fill_value =0)\n",
    "    df_end = df_end.reindex(pd.date_range(start=start_day,\n",
    "                                    end=end_day,freq='D'), fill_value =0)\n",
    "    \n",
    "    # compute daily stock\n",
    "    df_stock = df_start.cumsum() - df_end.cumsum()\n",
    "    \n",
    "    # add boundary conditions, if requested\n",
    "    if BOUNDARY == 'valid':\n",
    "        # TO BE REVIEWED\n",
    "        # start from the month after the first date\n",
    "        if not df_stock.index[0].is_month_start:\n",
    "            valid_start = pd.offsets.MonthBegin(0).rollforward(df_stock.index[0])\n",
    "        else:\n",
    "            valid_start = pd.offsets.MonthBegin(0).rollforward(df_stock.index[1])\n",
    "        print(f'Starting from {valid_start}')\n",
    "        df_stock = df_stock[df_stock.index>=valid_start]\n",
    "\n",
    "    # resample to monthly\n",
    "    stock_month1 = df_stock.resample('M').mean()\n",
    "    stock_month1.index = stock_month1.index.map(set_month_at_beginning)\n",
    "    \n",
    "    # Remove first month if we want to avoid the boundary\n",
    "    if BOUNDARY=='valid':\n",
    "        stock_month1 = stock_month1[stock_month1.index>=set_month_at_beginning(\n",
    "            pd.to_datetime(FIRST_VALID_MONTH))]\n",
    "    return stock_month1, df_stock, df_start, df_end\n",
    "\n",
    "\n",
    "def get_stock(data, agg_func = 'sum', agg_col = 'vacancy_weight', BOUNDARY = None):\n",
    "    \"\"\"Compute the daily stock of vacancies via cumulative sum of net Flow.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- dataframe with online job vacancies. Need to have \"date\", \"end_date\" and agg_col columns\n",
    "    agg_func: whether to count the vacancies or to sum the weights\n",
    "    agg_col -- reference column to aggregate (usually column with per-vacancy weights)\n",
    "    BOUNDARY -- what to do wrt boundary conditions (start and end month)\n",
    "    \"\"\"\n",
    "    \n",
    "    start_day = data.date.min()\n",
    "    end_day = data.date.max()\n",
    "    \n",
    "    if agg_func == 'sum':\n",
    "        vacancy_flow_per_day = data.groupby('date')[agg_col].sum()\n",
    "        vacancy_remove_per_day = data.groupby('end_date')[agg_col].sum()\n",
    "    else:\n",
    "        vacancy_flow_per_day = data.groupby('date')[agg_col].count()\n",
    "        vacancy_remove_per_day = data.groupby('end_date')[agg_col].count()\n",
    "    \n",
    "    # shift vacancy_remove_per_day by one day since vacancies disappear the day after their expiration date\n",
    "    vacancy_remove_per_day = vacancy_remove_per_day.shift(1)\n",
    "    \n",
    "    # adjust so that they start and end on the same dates\n",
    "    vacancy_flow_per_day = vacancy_flow_per_day.reindex(pd.date_range(start=start_day,\n",
    "                                        end=end_day,freq='D'), fill_value =0)\n",
    "    vacancy_remove_per_day = vacancy_remove_per_day.reindex(pd.date_range(start=start_day,\n",
    "                                        end=end_day,freq='D'), fill_value =0)\n",
    "    \n",
    "    # compute the net Flow\n",
    "    net_flow = vacancy_flow_per_day.fillna(0) - vacancy_remove_per_day.fillna(0)\n",
    "    \n",
    "    # Get the daily stock\n",
    "    daily_stock = net_flow.cumsum()\n",
    "    \n",
    "    # Resample to monthly stock\n",
    "    monthly_stock = net_flow.resample('M').sum().cumsum()\n",
    "    monthly_stock.index = monthly_stock.index.map(set_month_at_beginning)\n",
    "    \n",
    "    # enforce boundary conditions\n",
    "    if BOUNDARY == 'valid':\n",
    "        monthly_stock = monthly_stock[monthly_stock.index>=set_month_at_beginning(\n",
    "            pd.to_datetime(FIRST_VALID_MONTH))]\n",
    "    return monthly_stock, daily_stock, vacancy_flow_per_day, vacancy_remove_per_day\n",
    "        \n",
    "def get_stock_breakdown(data, agg_func = 'sum', agg_col = 'vacancy_weight', \n",
    "                              breakdown_col = 'organization_industry_value', BOUNDARY = None):\n",
    "    \"\"\"Compute the daily stock of vacancies via cumulative sum of net Flow.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- dataframe with online job vacancies. Need to have \"date\", \"end_date\" and agg_col columns\n",
    "    agg_func: whether to count the vacancies or to sum the weights\n",
    "    agg_col -- reference column to aggregate (usually column with per-vacancy weights)\n",
    "    BOUNDARY -- what to do wrt boundary conditions (start and end month)\n",
    "    \"\"\"\n",
    "    \n",
    "    start_day = data.date.min()\n",
    "    end_day = data.date.max()\n",
    "    \n",
    "    if agg_func == 'sum':\n",
    "        vacancy_flow_per_day = data.groupby(['date',breakdown_col])[agg_col].sum()\n",
    "        vacancy_remove_per_day = data.groupby(['end_date',breakdown_col])[agg_col].sum()\n",
    "    else:\n",
    "        vacancy_flow_per_day = data.groupby(['date',breakdown_col])[agg_col].count()\n",
    "        vacancy_remove_per_day = data.groupby(['end_date',breakdown_col])[agg_col].count()\n",
    "    \n",
    "    vacancy_flow_per_day = vacancy_flow_per_day.unstack()\n",
    "    vacancy_remove_per_day = vacancy_remove_per_day.unstack()\n",
    "    \n",
    "    # shift vacancy_remove_per_day by one day since vacancies disappear the day after their expiration date\n",
    "    vacancy_remove_per_day = vacancy_remove_per_day.shift(1)\n",
    "    \n",
    "    # adjust so that they start and end on the same dates\n",
    "    vacancy_flow_per_day = vacancy_flow_per_day.reindex(pd.date_range(start=start_day,\n",
    "                                        end=end_day,freq='D'), fill_value =0)\n",
    "    vacancy_remove_per_day = vacancy_remove_per_day.reindex(pd.date_range(start=start_day,\n",
    "                                        end=end_day,freq='D'), fill_value =0)\n",
    "    \n",
    "    # compute the net Flow\n",
    "    net_flow = vacancy_flow_per_day.fillna(0) - vacancy_remove_per_day.fillna(0)\n",
    "    \n",
    "    # Get the daily stock\n",
    "    daily_stock = net_flow.cumsum()\n",
    "    \n",
    "    # Resample to monthly stock\n",
    "    monthly_stock = net_flow.resample('M').sum().cumsum()/2\n",
    "    monthly_stock.index = monthly_stock.index.map(set_month_at_beginning)\n",
    "    \n",
    "    # enforce boundary conditions\n",
    "    if BOUNDARY == 'valid':\n",
    "        monthly_stock = monthly_stock[monthly_stock.index>=set_month_at_beginning(\n",
    "            pd.to_datetime(FIRST_VALID_MONTH))]\n",
    "    return monthly_stock, daily_stock, vacancy_flow_per_day, vacancy_remove_per_day\n",
    "'''\n",
    "1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
